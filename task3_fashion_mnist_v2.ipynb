{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¬¸ì œ 3-2: Fashion MNIST ë¶„ë¥˜ (ë°˜ë³µ í™€ë“œì•„ì›ƒ 5íšŒ)\n",
    "\n",
    "## ê°œìš”\n",
    "- **ë°ì´í„°ì…‹**: PyTorch FashionMNIST (28Ã—28 grayscale, 10 í´ë˜ìŠ¤)\n",
    "- **ê²€ì¦ ë°©ë²•**: ë°˜ë³µ í™€ë“œì•„ì›ƒ 5íšŒ\n",
    "- **í‰ê°€ ì²™ë„**: Accuracy, F1 Score (Micro)\n",
    "\n",
    "## ë°ì´í„° ë¶„í•  ë°©ë²• (ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ë²„ì „)\n",
    "\n",
    "| ë‹¨ê³„ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| 1 | Train + Test ë³‘í•© (70,000ê°œ) |\n",
    "| 2 | ì „ì²´ë¥¼ ë¬´ì‘ìœ„ ì…”í”Œ |\n",
    "| 3 | Test = 50% (35,000ê°œ) |\n",
    "| 4 | ë‚˜ë¨¸ì§€ 50% ì¤‘ Train=49%, Val=1% |\n",
    "| 5 | ìœ„ ê³¼ì •ì„ 5íšŒ ë°˜ë³µ |\n",
    "\n",
    "### ì‹¤ì œ ë°ì´í„° í¬ê¸°\n",
    "- **Test**: 50% Ã— 70,000 = **35,000ê°œ**\n",
    "- **Train**: 49% Ã— 35,000 = **17,150ê°œ** (ì „ì²´ì˜ 24.5%)\n",
    "- **Val**: 1% Ã— 35,000 = **350ê°œ** (ì „ì²´ì˜ 0.5%)\n",
    "\n",
    "## Fashion MNIST í´ë˜ìŠ¤ (10ê°œ)\n",
    "| Label | Class | ì„¤ëª… |\n",
    "|-------|-------|------|\n",
    "| 0 | T-shirt/top | í‹°ì…”ì¸  |\n",
    "| 1 | Trouser | ë°”ì§€ |\n",
    "| 2 | Pullover | í’€ì˜¤ë²„ |\n",
    "| 3 | Dress | ë“œë ˆìŠ¤ |\n",
    "| 4 | Coat | ì½”íŠ¸ |\n",
    "| 5 | Sandal | ìƒŒë“¤ |\n",
    "| 6 | Shirt | ì…”ì¸  |\n",
    "| 7 | Sneaker | ìš´ë™í™” |\n",
    "| 8 | Bag | ê°€ë°© |\n",
    "| 9 | Ankle boot | ì•µí´ë¶€ì¸  |\n",
    "\n",
    "## êµ¬í˜„ ëª¨ë¸ (5ê°œ)\n",
    "\n",
    "### ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸\n",
    "1. **LeNet-5** (LeCun, 1998) - ìµœì´ˆì˜ ì„±ê³µì  CNN, ìˆ˜ì—…ì‹œê°„ì— ë°°ìš´ ëª¨ë¸\n",
    "\n",
    "### ê³ ê¸‰ ëª¨ë¸\n",
    "2. **VGGNet-style** (Simonyan, 2014) - ì‘ì€ 3Ã—3 ì»¤ë„ì„ ê¹Šê²Œ ìŒ“ì€ êµ¬ì¡°\n",
    "3. **ResNetCNN** (He, 2015) - Residual Connectionìœ¼ë¡œ ê¹Šì€ í•™ìŠµ\n",
    "4. **SEResNet** (Hu, 2017) - SE Attention + Residual ê²°í•©\n",
    "5. **ConvNeXt-Small** (Liu, 2022) - ViTì˜ ì„¤ê³„ ì›ì¹™ì„ CNNì— ì ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ê³µìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ìœ í‹¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# - torch/torchvision: ëª¨ë¸ ì •ì˜, ë°ì´í„°ì…‹ ë¡œë“œ, ë³€í™˜\n",
    "# - sklearn: ë°ì´í„° ë¶„í•  ë° ì§€í‘œ ê³„ì‚°\n",
    "# - matplotlib: ì‹œê°í™” ë° í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "# -------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torch.amp import autocast, GradScaler  # Mixed Precision (ìƒˆ API)\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (Linux ì„œë²„ í™˜ê²½) - ë¼ë²¨ ê¹¨ì§ ë°©ì§€\n",
    "def set_korean_font():\n",
    "    \"\"\"ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë¥¼ ìë™ ì„¤ì •\"\"\"\n",
    "    font_candidates = [\n",
    "        'NanumGothic', 'NanumBarunGothic', 'Malgun Gothic',\n",
    "        'AppleGothic', 'DejaVu Sans', 'Noto Sans CJK KR'\n",
    "    ]\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            plt.rcParams['font.family'] = font\n",
    "            plt.rcParams['axes.unicode_minus'] = False\n",
    "            print(f\"í•œê¸€ í°íŠ¸ ì„¤ì •: {font}\")\n",
    "            return\n",
    "    \n",
    "    print(\"í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ë¬¸ ë¼ë²¨ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "set_korean_font()\n",
    "\n",
    "# GPU ìš°ì„ , Macì´ë©´ MPS, ê·¸ ì™¸ CPU ì„ íƒ\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# GPU ì •ë³´ ì¶œë ¥ (ë¦¬í¬íŠ¸ìš©)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# H100 GPU ìµœì í™” ê¸°ë³¸ ì„¤ì • + ì‹œë“œ ê³ ì • í—¬í¼\n",
    "# --------------------------------------------\n",
    "BATCH_SIZE = 256          # H100: ì‘ì€ ì´ë¯¸ì§€ì´ë¯€ë¡œ ë°°ì¹˜ í™•ì¥\n",
    "NUM_WORKERS = 8           # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”\n",
    "PIN_MEMORY = True         # GPU ë©”ëª¨ë¦¬ ì „ì†¡ ìµœì í™”\n",
    "USE_AMP = True            # Mixed Precision (BF16/FP16)\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"torch/np ì‹œë“œë¥¼ ëª¨ë‘ ê³ ì •í•˜ì—¬ ë°˜ë³µ í™€ë“œì•„ì›ƒ ì¼ê´€ì„± ìœ ì§€\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ê³ ì • ì…ë ¥ í¬ê¸°ì—ì„œ ì„±ëŠ¥ í–¥ìƒ\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"H100 ìµœì í™” ì„¤ì •:\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Workers: {NUM_WORKERS}\")\n",
    "print(f\"  - Pin Memory: {PIN_MEMORY}\")\n",
    "print(f\"  - Mixed Precision (AMP): {USE_AMP}\")\n",
    "print(f\"  - cuDNN Benchmark: {torch.backends.cudnn.benchmark}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Fashion MNIST ë‹¤ìš´ë¡œë“œ ë° ê¸°ë³¸ ë³€í™˜\n",
    "# - í‘ë°± ì´ë¯¸ì§€ì´ë¯€ë¡œ 1ì±„ë„ ì •ê·œí™” ì‚¬ìš©\n",
    "# --------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "### ë¶„í•  ì „ëµ (ì´ ë²„ì „)\n",
    "1. Train + Test ë³‘í•© â†’ 70,000ê°œ\n",
    "2. ì „ì²´ë¥¼ ë¬´ì‘ìœ„ ì…”í”Œ\n",
    "3. **Test = 50%** (35,000ê°œ)\n",
    "4. **ë‚˜ë¨¸ì§€ 50% ì¤‘**: Train = 49%, Val = 1%\n",
    "   - Train: 17,150ê°œ (ì „ì²´ì˜ 24.5%)\n",
    "   - Val: 350ê°œ (ì „ì²´ì˜ 0.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ì£¼ì–´ì§„ train/testë¥¼ í•©ì³ì„œ ìƒˆë¡œìš´ ë¶„í•  ì¤€ë¹„\n",
    "# ë¶„í• : Test 50% â†’ ë‚˜ë¨¸ì§€ ì¤‘ Train 49%, Val 1%\n",
    "# --------------------------------------------\n",
    "full_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "print(f\"ì „ì²´ ë°ì´í„°: {len(full_dataset)}\")\n",
    "\n",
    "# ë ˆì´ë¸” ì¶”ì¶œ (ConcatDataset ë‚´ë¶€ ìˆœíšŒ)\n",
    "all_labels = []\n",
    "for ds in full_dataset.datasets:\n",
    "    all_labels.extend(ds.targets.numpy())\n",
    "all_labels = np.array(all_labels)\n",
    "all_indices = np.arange(len(full_dataset))\n",
    "\n",
    "# ë¶„í•  ë¹„ìœ¨ ê³„ì‚° ë° ì¶œë ¥\n",
    "total = len(full_dataset)\n",
    "test_size = int(total * 0.5)  # 50%\n",
    "remaining = total - test_size\n",
    "train_size = int(remaining * 0.49)  # ë‚˜ë¨¸ì§€ì˜ 49%\n",
    "val_size = remaining - train_size   # ë‚˜ë¨¸ì§€ì˜ 1%\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¶„í•  ê³„íš:\")\n",
    "print(f\"  - Test:  {test_size:,}ê°œ ({test_size/total*100:.1f}%)\")\n",
    "print(f\"  - Train: {train_size:,}ê°œ ({train_size/total*100:.1f}%)\")\n",
    "print(f\"  - Val:   {val_size:,}ê°œ ({val_size/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ìƒ˜í”Œ ì‹œê°í™” - í´ë˜ìŠ¤ ë¼ë²¨ í™•ì¸\n",
    "# --------------------------------------------\n",
    "class_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ì •ì˜\n",
    "\n",
    "### ë§ êµ¬ì¡° í‘œê¸°ë²•\n",
    "- **C**: Convolution (í•©ì„±ê³±) - ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ\n",
    "- **BN**: Batch Normalization - í•™ìŠµ ì•ˆì •í™”, ìˆ˜ë ´ ê°€ì†\n",
    "- **R**: ReLU / **T**: Tanh - í™œì„±í™” í•¨ìˆ˜\n",
    "- **AvgP** / **MaxP**: Average/Max Pooling - ë‹¤ìš´ìƒ˜í”Œë§\n",
    "- **GAP**: Global Average Pooling - íŒŒë¼ë¯¸í„° ê°ì†Œ\n",
    "- **D**: Dropout - ì •ê·œí™”, ê³¼ì í•© ë°©ì§€\n",
    "- **FC**: Fully Connected - ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë³€í™˜\n",
    "- **Flat**: Flatten - 2Dâ†’1D ë³€í™˜\n",
    "- **Res**: Residual Connection - skip connection\n",
    "- **SE**: Squeeze-and-Excitation - ì±„ë„ë³„ attention\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 LeNet-5 (ë² ì´ìŠ¤ë¼ì¸, LeCun et al., 1998)\n",
    "\n",
    "**êµ¬ì¡°**: `C(1,6,5Ã—5) â†’ T â†’ AvgP(2Ã—2) â†’ C(6,16,5Ã—5) â†’ T â†’ AvgP(2Ã—2) â†’ Flat â†’ FC(256,120) â†’ T â†’ FC(120,84) â†’ T â†’ FC(84,10)`\n",
    "\n",
    "**íŠ¹ì§•**:\n",
    "- ìµœì´ˆì˜ ì„±ê³µì ì¸ CNN êµ¬ì¡° (ìˆ˜ì—…ì‹œê°„ì— ë°°ìš´ ëª¨ë¸)\n",
    "- 5Ã—5 í° ì»¤ë„ë¡œ íŠ¹ì§• ì¶”ì¶œ\n",
    "- Average Poolingìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§\n",
    "- Tanh í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš© (ì›ë³¸ ë…¼ë¬¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 (LeCun et al., 1998)\n",
    "    êµ¬ì¡°: C(1,6,5x5) â†’ T â†’ AvgP â†’ C(6,16,5x5) â†’ T â†’ AvgP â†’ Flat â†’ FC â†’ T â†’ FC â†’ T â†’ FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(1, 6, 5, 1, 2),         # C(1,6,5x5): 28x28 â†’ 28x28\n",
    "            nn.Tanh(),                         # T: í™œì„±í™” (ì›ë³¸)\n",
    "            nn.AvgPool2d(2, 2),                # AvgP: 28x28 â†’ 14x14\n",
    "            nn.Conv2d(6, 16, 5),               # C(6,16,5x5): 14x14 â†’ 10x10\n",
    "            nn.Tanh(),                         # T: í™œì„±í™”\n",
    "            nn.AvgPool2d(2, 2),                # AvgP: 10x10 â†’ 5x5\n",
    "            nn.Flatten(),                      # Flat: (B,16,5,5) â†’ (B,400)\n",
    "            nn.Linear(16 * 5 * 5, 120),        # FC(400,120)\n",
    "            nn.Tanh(),                         # T: í™œì„±í™”\n",
    "            nn.Linear(120, 84),                # FC(120,84)\n",
    "            nn.Tanh(),                         # T: í™œì„±í™”\n",
    "            nn.Linear(84, num_classes)         # FC(84,10): ì¶œë ¥ì¸µ\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"LeNet-5: C(5x5) â†’ T â†’ AvgP â†’ C(5x5) â†’ T â†’ AvgP â†’ Flat â†’ FC â†’ T â†’ FC â†’ T â†’ FC\")\n",
    "print(LeNet5())\n",
    "print(f\"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in LeNet5().parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 VGGNet-style (Simonyan & Zisserman, 2014)\n",
    "\n",
    "**êµ¬ì¡°**: `[C(3Ã—3) â†’ BN â†’ R]Ã—2 â†’ MaxP â†’ [C(3Ã—3) â†’ BN â†’ R]Ã—2 â†’ MaxP â†’ [C(3Ã—3) â†’ BN â†’ R]Ã—2 â†’ GAP â†’ D â†’ FC`\n",
    "\n",
    "**íŠ¹ì§•**:\n",
    "- ì‘ì€ 3Ã—3 ì»¤ë„ì„ ê¹Šê²Œ ìŒ“ìŒ (VGGì˜ í•µì‹¬ ì•„ì´ë””ì–´)\n",
    "- 3Ã—3 ë‘ ë²ˆ = 5Ã—5 ìˆ˜ìš© ì˜ì—­, íŒŒë¼ë¯¸í„°ëŠ” ë” ì ìŒ\n",
    "- Batch Normalization + ReLUë¡œ í˜„ëŒ€í™”\n",
    "- MaxPoolingìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG Block: 3x3 Convë¥¼ ì—¬ëŸ¬ ë²ˆ ìŒ“ì€ ë¸”ë¡\n",
    "    êµ¬ì¡°: [C(3x3) â†’ BN â†’ R] Ã— n_convs\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, n_convs=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_convs):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_ch if i == 0 else out_ch, out_ch, 3, 1, 1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    VGGNet-style (Simonyan & Zisserman, 2014)\n",
    "    êµ¬ì¡°: VGGBlockÃ—3 + MaxPoolÃ—2 + GAP + FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            VGGBlock(1, 32, n_convs=2),            # Block1: 28x28 â†’ 28x28\n",
    "            nn.MaxPool2d(2, 2),                     # MaxP: 28 â†’ 14\n",
    "            VGGBlock(32, 64, n_convs=2),           # Block2: 14x14 â†’ 14x14\n",
    "            nn.MaxPool2d(2, 2),                     # MaxP: 14 â†’ 7\n",
    "            VGGBlock(64, 128, n_convs=2),          # Block3: 7x7 â†’ 7x7\n",
    "            nn.AdaptiveAvgPool2d(1),               # GAP: 7x7 â†’ 1x1\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"VGGNet: [C(3x3)â†’BNâ†’R]Ã—2 â†’ MaxP â†’ [C(3x3)â†’BNâ†’R]Ã—2 â†’ MaxP â†’ [C(3x3)â†’BNâ†’R]Ã—2 â†’ GAP â†’ D â†’ FC\")\n",
    "print(VGGNet())\n",
    "print(f\"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in VGGNet().parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ResNetCNN (He et al., 2015)\n",
    "\n",
    "**êµ¬ì¡°**: `C(1,32) â†’ BN â†’ R â†’ [ResBlock(32,32)]Ã—1 â†’ [ResBlock(32,64)]Ã—1 â†’ [ResBlock(64,128)]Ã—1 â†’ GAP â†’ D(0.5) â†’ FC(128,10)`\n",
    "\n",
    "**ResBlock ë‚´ë¶€**: `C(3Ã—3) â†’ BN â†’ R â†’ C(3Ã—3) â†’ BN â†’ (+shortcut) â†’ R`\n",
    "\n",
    "**íŠ¹ì§•**:\n",
    "- Residual Connection (skip connection) ë„ì…\n",
    "- ì…ë ¥ì„ ì¶œë ¥ì— ë”í•¨: H(x) = F(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block (He et al., 2015)\n",
    "    êµ¬ì¡°: C â†’ BN â†’ R â†’ C â†’ BN â†’ (+shortcut) â†’ R\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        ])\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out += self.shortcut(x)\n",
    "        return nn.ReLU()(out)\n",
    "\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNetCNN\n",
    "    êµ¬ì¡°: C(1,32) â†’ BN â†’ R â†’ Res(32,32) â†’ Res(32,64) â†’ Res(64,128) â†’ GAP â†’ D â†’ FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(1, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(32, 32, 1),\n",
    "            ResidualBlock(32, 64, 2),\n",
    "            ResidualBlock(64, 128, 2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"ResNetCNN: C â†’ BN â†’ R â†’ Res â†’ Res â†’ Res â†’ GAP â†’ D â†’ FC\")\n",
    "print(ResNetCNN())\n",
    "print(f\"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in ResNetCNN().parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 SEResNet (Hu et al., 2017)\n",
    "\n",
    "**êµ¬ì¡°**: `C(1,32) â†’ BN â†’ R â†’ [SEResBlock]Ã—3 â†’ GAP â†’ D(0.5) â†’ FC(128,10)`\n",
    "\n",
    "**SE Block**: `GAP â†’ FC(C,C/16) â†’ R â†’ FC(C/16,C) â†’ Sigmoid â†’ Scale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block (Hu et al., 2017)\n",
    "    êµ¬ì¡°: GAP â†’ FC(C, C/r) â†’ R â†’ FC(C/r, C) â†’ Sigmoid â†’ Scale\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        scale = x\n",
    "        for layer in self.layers:\n",
    "            scale = layer(scale)\n",
    "        scale = scale.view(b, c, 1, 1)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class SEResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SE-Residual Block\n",
    "    êµ¬ì¡°: C â†’ BN â†’ R â†’ C â†’ BN â†’ SE â†’ (+shortcut) â†’ R\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            SEBlock(out_ch)\n",
    "        ])\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out += self.shortcut(x)\n",
    "        return nn.ReLU()(out)\n",
    "\n",
    "\n",
    "class SEResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SE-ResNet (Squeeze-and-Excitation ResNet)\n",
    "    êµ¬ì¡°: C(1,32) â†’ BN â†’ R â†’ SERes(32,32) â†’ SERes(32,64) â†’ SERes(64,128) â†’ GAP â†’ D â†’ FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(1, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            SEResidualBlock(32, 32, 1),\n",
    "            SEResidualBlock(32, 64, 2),\n",
    "            SEResidualBlock(64, 128, 2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"SEResNet: C â†’ BN â†’ R â†’ SERes â†’ SERes â†’ SERes â†’ GAP â†’ D â†’ FC\")\n",
    "print(SEResNet())\n",
    "print(f\"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in SEResNet().parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 ConvNeXt-Small (Liu et al., 2022)\n",
    "\n",
    "**í•µì‹¬ ì•„ì´ë””ì–´**: Vision Transformer(ViT)ì˜ ì„¤ê³„ ì›ì¹™ì„ ìˆœìˆ˜ CNNì— ì ìš©\n",
    "\n",
    "**ConvNeXt Block**: `DWConv(7Ã—7) â†’ LayerNorm â†’ PWConv(1Ã—1, 4Ã—í™•ì¥) â†’ GELU â†’ PWConv(1Ã—1, ì¶•ì†Œ) â†’ LayerScale â†’ + Residual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ConvNeXt-Small (Fashion MNIST, 28x28 Gray)\n",
    "# - Depthwise 7x7 â†’ LayerNorm â†’ Pointwise í™•ì¥/ì¶•ì†Œ (GELU)\n",
    "# - 3-stage ê²½ëŸ‰ ConvNeXt: {2,2,6} ë¸”ë¡, í•´ìƒë„ ë‹¨ê³„ë³„ ì ˆë°˜\n",
    "# --------------------------------------------\n",
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"ì±„ë„ ê¸°ì¤€ LayerNorm (ConvNeXt ìŠ¤íƒ€ì¼)\"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight.view(1, -1, 1, 1) * x + self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    \"\"\"ConvNeXt Block: DWConv(7x7) â†’ LN â†’ PWConvÃ—2 + GELU â†’ LayerScale â†’ Residual\"\"\"\n",
    "    def __init__(self, dim, layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.norm = LayerNorm2d(dim)\n",
    "        self.pwconv1 = nn.Conv2d(dim, 4 * dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(4 * dim, dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim)) if layer_scale_init_value > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma.view(1, -1, 1, 1) * x\n",
    "        return shortcut + x\n",
    "\n",
    "\n",
    "class ConvNeXtSmallFashion(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt-Small ë³€í˜• (28x28, 1ì±„ë„)\n",
    "    êµ¬ì¡°: Stem(4x4,s4) â†’ [BlockÃ—{2,2,6} + Downsample] â†’ GAP â†’ Dropout â†’ FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        depths = [2, 2, 6]\n",
    "        dims = [64, 128, 256]\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, dims[0], kernel_size=4, stride=4),  # 28 â†’ 7\n",
    "                LayerNorm2d(dims[0])\n",
    "            )\n",
    "        ])\n",
    "        for i in range(2):\n",
    "            self.downsample_layers.append(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(dims[i]),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2)  # 7â†’4â†’2\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            blocks = [ConvNeXtBlock(dims[i]) for _ in range(depths[i])]\n",
    "            self.stages.append(nn.ModuleList(blocks))\n",
    "\n",
    "        self.norm = LayerNorm2d(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample_layers[0](x)\n",
    "        for i in range(3):\n",
    "            for block in self.stages[i]:\n",
    "                x = block(x)\n",
    "            if i < 2:\n",
    "                x = self.downsample_layers[i+1](x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean([-2, -1])\n",
    "        x = self.dropout(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"ConvNeXt-Small (Fashion): Stem(4x4,s4) â†’ BlocksÃ—{2,2,6} â†’ GAP â†’ D â†’ FC\")\n",
    "print(ConvNeXtSmallFashion())\n",
    "print(f\"íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in ConvNeXtSmallFashion().parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# í•™ìŠµ/í‰ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (H100 ìµœì í™”)\n",
    "# --------------------------------------------\n",
    "scaler = GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"Mixed Precision í•™ìŠµ (H100 ìµœì í™”)\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast('cuda', enabled=USE_AMP):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"í‰ê°€ í•¨ìˆ˜ - Accuracy, F1 (Micro) ë°˜í™˜\"\"\"\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            with autocast('cuda', enabled=USE_AMP):\n",
    "                outputs = model(images)\n",
    "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "    \n",
    "    acc = accuracy_score(labels_list, preds)\n",
    "    f1_micro = f1_score(labels_list, preds, average='micro')\n",
    "    return acc, f1_micro\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=30, lr=0.001):\n",
    "    \"\"\"H100 ìµœì í™” í•™ìŠµ í•¨ìˆ˜\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, _ = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 7:\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "print(\"H100 ìµœì í™” í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"  - í‰ê°€ ì§€í‘œ: Accuracy, F1 Score (Micro)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë°˜ë³µ í™€ë“œì•„ì›ƒ ê²€ì¦ (5íšŒ)\n",
    "\n",
    "### ë°ì´í„° ë¶„í•  ë°©ë²•\n",
    "1. Train + Test ë³‘í•© (70,000ê°œ)\n",
    "2. ì „ì²´ë¥¼ ë¬´ì‘ìœ„ ì…”í”Œ\n",
    "3. **Test = 50%** (35,000ê°œ)\n",
    "4. ë‚˜ë¨¸ì§€ 50% ì¤‘ **Train = 49%, Val = 1%**\n",
    "5. ìœ„ ê³¼ì •ì„ **5íšŒ ë°˜ë³µ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ë°˜ë³µ í™€ë“œì•„ì›ƒ ì‹¤í–‰ í•¨ìˆ˜\n",
    "# ë¶„í• : Test 50% â†’ ë‚˜ë¨¸ì§€ ì¤‘ Train 49%, Val 1% (5íšŒ ë°˜ë³µ)\n",
    "# --------------------------------------------\n",
    "def run_holdout_v2(model_class, model_name, n_repeats=5):\n",
    "    \"\"\"H100 ìµœì í™” ë°˜ë³µ í™€ë“œì•„ì›ƒ (Test 50%, Train 49%, Val 1%)\"\"\"\n",
    "    results = {'accuracy': [], 'f1_micro': []}\n",
    "    \n",
    "    print()\n",
    "    print('='*60)\n",
    "    print(model_name)\n",
    "    print('='*60)\n",
    "    \n",
    "    for i in range(n_repeats):\n",
    "        set_seed(42 + i)\n",
    "        \n",
    "        # 1ë‹¨ê³„: Test 50% ë¶„ë¦¬\n",
    "        remaining_idx, test_idx = train_test_split(\n",
    "            all_indices, \n",
    "            test_size=0.5,  # Test = 50%\n",
    "            stratify=all_labels, \n",
    "            random_state=42+i\n",
    "        )\n",
    "        \n",
    "        # 2ë‹¨ê³„: ë‚˜ë¨¸ì§€ 50%ë¥¼ Train 49%, Val 1%ë¡œ ë¶„í• \n",
    "        # (ë‚˜ë¨¸ì§€ì˜ 49% = ì „ì²´ì˜ 24.5%, ë‚˜ë¨¸ì§€ì˜ 1% = ì „ì²´ì˜ 0.5%)\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            remaining_idx, \n",
    "            test_size=1/50,  # Val = ë‚˜ë¨¸ì§€ì˜ 1/50 â‰ˆ 2% â†’ ì‹¤ì œë¡œëŠ” 1% of 50% = 0.5%\n",
    "            stratify=all_labels[remaining_idx], \n",
    "            random_state=42+i\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„° í¬ê¸° ì¶œë ¥ (ì²« ë°˜ë³µë§Œ)\n",
    "        if i == 0:\n",
    "            print(f\"ë°ì´í„° ë¶„í• : Train={len(train_idx):,}, Val={len(val_idx):,}, Test={len(test_idx):,}\")\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            Subset(full_dataset, train_idx), \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            Subset(full_dataset, val_idx), \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            Subset(full_dataset, test_idx), \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "        \n",
    "        model = model_class().to(device)\n",
    "        \n",
    "        if hasattr(torch, 'compile'):\n",
    "            try:\n",
    "                model = torch.compile(model, mode='reduce-overhead')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        model = train_model(model, train_loader, val_loader, epochs=30)\n",
    "        \n",
    "        acc, f1_micro = evaluate(model, test_loader)\n",
    "        results['accuracy'].append(acc)\n",
    "        results['f1_micro'].append(f1_micro)\n",
    "        print(f\"[{i+1}/{n_repeats}] Acc: {acc:.4f}, F1(Micro): {f1_micro:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_results = run_holdout_v2(LeNet5, \"LeNet-5 (ë² ì´ìŠ¤ë¼ì¸)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_results = run_holdout_v2(VGGNet, \"VGGNet-style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_results = run_holdout_v2(ResNetCNN, \"ResNetCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senet_results = run_holdout_v2(SEResNet, \"SEResNet (ê³ ì„±ëŠ¥)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnext_results = run_holdout_v2(ConvNeXtSmallFashion, \"ConvNeXt-Small (ê³ ì„±ëŠ¥)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(results, name):\n",
    "    # í‰ê· /í‘œì¤€í¸ì°¨ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•œ í—¬í¼\n",
    "    acc_mean, acc_std = np.mean(results['accuracy']), np.std(results['accuracy'])\n",
    "    f1_micro_mean, f1_micro_std = np.mean(results['f1_micro']), np.std(results['f1_micro'])\n",
    "    print(name)\n",
    "    print(f\"  Accuracy:    {acc_mean:.4f} Â± {acc_std:.4f}\")\n",
    "    print(f\"  F1 (Micro):  {f1_micro_mean:.4f} Â± {f1_micro_std:.4f}\")\n",
    "    return acc_mean, acc_std, f1_micro_mean, f1_micro_std\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Final Results (Test 50%, Train 49%, Val 1%)\")\n",
    "print(\"=\"*60)\n",
    "lenet_stats = print_summary(lenet_results, \"LeNet-5\")\n",
    "vgg_stats = print_summary(vgg_results, \"VGGNet\")\n",
    "resnet_stats = print_summary(resnet_results, \"ResNetCNN\")\n",
    "senet_stats = print_summary(senet_results, \"SEResNet\")\n",
    "convnext_stats = print_summary(convnext_results, \"ConvNeXt-Small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ì‹œê°í™” - Accuracy, F1 (Micro) ë¹„êµ\n",
    "# - í•œê¸€ í°íŠ¸ ì¬ì„¤ì • (ê·¸ë˜í”„ ì¶œë ¥ ì „ í™•ì¸)\n",
    "# --------------------------------------------\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì¬ì„¤ì • (ì‹œê°í™” ì „ í™•ì¸)\n",
    "import platform\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "else:  # Linux\n",
    "    font_list = [f.name for f in fm.fontManager.ttflist if 'Nanum' in f.name or 'Gothic' in f.name]\n",
    "    if font_list:\n",
    "        plt.rcParams['font.family'] = font_list[0]\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ëª¨ë¸ë³„ ê²°ê³¼ ì •ë¦¬\n",
    "models = ['LeNet-5', 'VGGNet', 'ResNetCNN', 'SEResNet', 'ConvNeXt']\n",
    "all_results = [lenet_results, vgg_results, resnet_results, senet_results, convnext_results]\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2a3ff']\n",
    "\n",
    "# ============================================\n",
    "# ê·¸ë˜í”„ 1: Accuracyì™€ F1 (Micro) ë¹„êµ (í‰ê·  Â± í‘œì¤€í¸ì°¨)\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Accuracy\n",
    "acc_means = [np.mean(r['accuracy']) for r in all_results]\n",
    "acc_stds = [np.std(r['accuracy']) for r in all_results]\n",
    "bars1 = axes[0].bar(models, acc_means, yerr=acc_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title('Accuracy (ì •í™•ë„)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0.7, 1.0])\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "for bar, mean, std in zip(bars1, acc_means, acc_stds):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.005, \n",
    "                 f'{mean:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. F1 Score (Micro)\n",
    "f1_micro_means = [np.mean(r['f1_micro']) for r in all_results]\n",
    "f1_micro_stds = [np.std(r['f1_micro']) for r in all_results]\n",
    "bars2 = axes[1].bar(models, f1_micro_means, yerr=f1_micro_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_title('F1 Score (Micro)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0.7, 1.0])\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "for bar, mean, std in zip(bars2, f1_micro_means, f1_micro_stds):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.005, \n",
    "                 f'{mean:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Fashion MNIST V2 - ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (5íšŒ ë°˜ë³µ, Test 50%)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('task3_v2_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ê·¸ë˜í”„ 2: ë‘ ì§€í‘œë¥¼ í•˜ë‚˜ì˜ ê·¸ë˜í”„ì— ê·¸ë£¹ìœ¼ë¡œ í‘œì‹œ\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars_acc = ax.bar(x - width/2, acc_means, width, yerr=acc_stds, label='Accuracy', color='#4ECDC4', capsize=3, edgecolor='black', linewidth=0.5)\n",
    "bars_micro = ax.bar(x + width/2, f1_micro_means, width, yerr=f1_micro_stds, label='F1 (Micro)', color='#FF6B6B', capsize=3, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('ëª¨ë¸', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Fashion MNIST V2 - í‰ê°€ ì§€í‘œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\\n(Test 50%, Train 49%, Val 1%, 5íšŒ ë°˜ë³µ)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=10)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.set_ylim([0.7, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, mean in zip(bars_acc, acc_means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.015, \n",
    "            f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "for bar, mean in zip(bars_micro, f1_micro_means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.015, \n",
    "            f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task3_v2_grouped_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ê·¸ë˜í”„ 3: ë°˜ë³µë³„ Accuracy ì„±ëŠ¥ ë¹„êµ\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "x = np.arange(5)\n",
    "width = 0.16\n",
    "\n",
    "for i, (result, model, color) in enumerate(zip(all_results, models, colors)):\n",
    "    offset = (i - 2) * width\n",
    "    ax.bar(x + offset, result['accuracy'], width, label=model, color=color, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('ë°˜ë³µ íšŸìˆ˜ (Iteration)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Fashion MNIST V2 - ë°˜ë³µë³„ Accuracy ì„±ëŠ¥', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{i+1}íšŒ' for i in x], fontsize=10)\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.set_ylim([0.7, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task3_v2_iteration_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ê²°ê³¼ ìš”ì•½ í…Œì´ë¸” ì¶œë ¥\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š Fashion MNIST V2 - ìµœì¢… ê²°ê³¼ ìš”ì•½ (5íšŒ ë°˜ë³µ í™€ë“œì•„ì›ƒ)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ë°ì´í„° ë¶„í• : Test 50%, Train 49% (of remaining), Val 1% (of remaining)\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'ëª¨ë¸':<15} {'Accuracy':<25} {'F1 (Micro)':<25}\")\n",
    "print(\"-\"*70)\n",
    "for model, result in zip(models, all_results):\n",
    "    acc_m, acc_s = np.mean(result['accuracy']), np.std(result['accuracy'])\n",
    "    f1mi_m, f1mi_s = np.mean(result['f1_micro']), np.std(result['f1_micro'])\n",
    "    print(f\"{model:<15} {acc_m:.4f} Â± {acc_s:.4f}           {f1mi_m:.4f} Â± {f1mi_s:.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâ€» ì°¸ê³ : ê· í˜• ë°ì´í„°ì…‹ì—ì„œ F1 (Micro) = Accuracy ì…ë‹ˆë‹¤.\")\n",
    "print(\"â€» 10ê°œ í´ë˜ìŠ¤: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. ê²°ë¡ \n\n### V1 vs V2 ë°ì´í„° ë¶„í•  ë¹„êµ\n\n| í•­ëª© | V1 (ê¸°ì¡´) | V2 (ì´ ë²„ì „) |\n|------|----------|-------------|\n| **ë¶„í•  ìˆœì„œ** | Train ë¨¼ì € | Test ë¨¼ì € |\n| **Train** | 2% (1,400ê°œ) | 24.5% (17,150ê°œ) |\n| **Val** | 49% (34,300ê°œ) | 0.5% (350ê°œ) |\n| **Test** | 49% (34,300ê°œ) | 50% (35,000ê°œ) |\n| **ë°˜ë³µ íšŸìˆ˜** | 10íšŒ | 5íšŒ |\n| **í‰ê°€ ì§€í‘œ** | Acc, F1(Micro), F1(Macro) | Acc, F1(Micro) |\n\n---\n\n### V2 ë¶„í• ì˜ íŠ¹ì§•\n\n#### 1. ë” ë§ì€ í›ˆë ¨ ë°ì´í„° (12ë°° ì¦ê°€)\n- **V1**: 1,400ê°œ â†’ **V2**: 17,150ê°œ\n- ëª¨ë¸ì´ ë” ë§ì€ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ\n- ê³¼ì í•© ìœ„í—˜ ê°ì†Œ\n- ë³µì¡í•œ ëª¨ë¸(ConvNeXt)ì— ìœ ë¦¬\n\n#### 2. ì‘ì€ ê²€ì¦ ì„¸íŠ¸\n- **V1**: 34,300ê°œ â†’ **V2**: 350ê°œ\n- ê²€ì¦ ì„±ëŠ¥ì˜ ë¶„ì‚°ì´ ì»¤ì§ˆ ìˆ˜ ìˆìŒ\n- Early Stoppingì´ ëœ ì•ˆì •ì ì¼ ìˆ˜ ìˆìŒ\n- ê²€ì¦ ì„¸íŠ¸ê°€ ë„ˆë¬´ ì‘ì•„ ê³¼ì í•© ê°ì§€ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ\n\n#### 3. í° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸\n- **V1**: 34,300ê°œ â†’ **V2**: 35,000ê°œ\n- ìµœì¢… ì„±ëŠ¥ í‰ê°€ê°€ ë” ì•ˆì •ì \n- í†µê³„ì ìœ¼ë¡œ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼\n\n---\n\n### ëª¨ë¸ë³„ ì„±ëŠ¥ ì˜ˆìƒ\n\n| ëª¨ë¸ | ì—°ë„ | í•µì‹¬ êµ¬ì¡° | íŒŒë¼ë¯¸í„° | V2 ì˜ˆìƒ ì„±ëŠ¥ |\n|------|------|----------|---------|-------------|\n| LeNet-5 | 1998 | C(5Ã—5)â†’Tanhâ†’AvgP | ~62K | ë‚®ìŒ |\n| VGGNet | 2014 | [C(3Ã—3)â†’BNâ†’R]Ã—nâ†’MaxP | ~168K | ì¤‘ê°„ |\n| ResNetCNN | 2015 | Residual Block + Skip | ~171K | ë†’ìŒ |\n| SEResNet | 2017 | SE Block + Residual | ~172K | ë†’ìŒ |\n| ConvNeXt-Small | 2022 | DWConv(7Ã—7) + LN + GELU | ~1.6M | ìµœê³  |\n\nâ€» ì‹¤ì œ ì„±ëŠ¥ì€ ìœ„ ì‹œê°í™” ê²°ê³¼ ì°¸ì¡°\nâ€» ê· í˜• ë°ì´í„°ì…‹ì—ì„œ F1 (Micro) = AccuracyëŠ” sklearnì˜ ì •ìƒ ë™ì‘ì…ë‹ˆë‹¤.\n\n---\n\n### Fashion MNIST í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ë‚œì´ë„\n\n| í´ë˜ìŠ¤ | êµ¬ë¶„ ë‚œì´ë„ | ìœ ì‚¬ í´ë˜ìŠ¤ |\n|--------|------------|------------|\n| Trouser (ë°”ì§€) | ì‰¬ì›€ | í˜•íƒœ ê³ ìœ  |\n| Bag (ê°€ë°©) | ì‰¬ì›€ | í˜•íƒœ ê³ ìœ  |\n| Sneaker (ìš´ë™í™”) | ì¤‘ê°„ | Ankle boot |\n| Sandal (ìƒŒë“¤) | ì¤‘ê°„ | í˜•íƒœ ê³ ìœ  |\n| Ankle boot (ì•µí´ë¶€ì¸ ) | ì¤‘ê°„ | Sneaker |\n| T-shirt (í‹°ì…”ì¸ ) | ì–´ë ¤ì›€ | Shirt, Pullover |\n| Shirt (ì…”ì¸ ) | ì–´ë ¤ì›€ | T-shirt, Pullover |\n| Pullover (í’€ì˜¤ë²„) | ì–´ë ¤ì›€ | Shirt, Coat |\n| Coat (ì½”íŠ¸) | ì–´ë ¤ì›€ | Pullover |\n| Dress (ë“œë ˆìŠ¤) | ì–´ë ¤ì›€ | Coat |\n\nâ†’ **ìƒì˜ ë¶„ë¥˜ (T-shirt, Shirt, Pullover, Coat)** ê°€ ê°€ì¥ ì–´ë ¤ì›€\n\n---\n\n### ê²°ë¡ \n\nV2 ë¶„í•  ë°©ì‹ì—ì„œëŠ” **í›ˆë ¨ ë°ì´í„°ê°€ 12ë°° ì¦ê°€**í•˜ì—¬:\n\n1. **ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ**: ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµí•˜ë¯€ë¡œ V1ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥\n2. **ConvNeXt ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ**: íŒŒë¼ë¯¸í„°ê°€ ë§ì€ ëª¨ë¸ì¼ìˆ˜ë¡ ë” í° ê°œì„  ì˜ˆìƒ\n3. **Early Stopping ë¶ˆì•ˆì •**: ì‘ì€ ê²€ì¦ ì„¸íŠ¸ë¡œ ì¸í•´ ìµœì  ì‹œì  íŒë‹¨ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ\n\n**V2ì—ì„œ ìµœê³  ì„±ëŠ¥ ì˜ˆìƒ ëª¨ë¸**:\n\n1. **ConvNeXt-Small**: \n   - ê°€ì¥ ë§ì€ íŒŒë¼ë¯¸í„°ë¡œ ì¦ê°€ëœ í›ˆë ¨ ë°ì´í„°ë¥¼ ìµœëŒ€ë¡œ í™œìš©\n   - 7Ã—7 Depthwiseë¡œ ì˜ë¥˜ ì „ì²´ í˜•íƒœ ì¸ì‹\n   - ìµœì‹  ì„¤ê³„ ì›ì¹™ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ\n\n2. **SEResNet**: \n   - ì±„ë„ Attentionìœ¼ë¡œ ìƒì˜ ê°„ ë¯¸ì„¸í•œ ì°¨ì´ êµ¬ë¶„ì— ìœ ë¦¬\n   - ì ì€ íŒŒë¼ë¯¸í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥\n\n**ì¶”ì²œ**: í›ˆë ¨ ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ì•„ì¡Œìœ¼ë¯€ë¡œ, **ë³µì¡í•œ ëª¨ë¸(SEResNet, ConvNeXt)**ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}