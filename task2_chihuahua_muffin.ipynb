{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 문제 2: 치와와 vs 머핀 분류\n\n- **데이터셋**: Kaggle Muffin vs Chihuahua (128×128 RGB)\n- **검증**: StratifiedKFold 5겹 교차검증\n- **평가**: Accuracy, F1 Score (Micro)\n\n## 모델 구조\n1. **LeNet-5** (베이스라인) - 수업시간에 배운 고전적 CNN\n2. **VGGNet-style** - 작은 3×3 커널을 깊게 쌓은 구조\n3. **ResNetCNN** - Residual Connection으로 깊은 학습\n4. **EfficientNet-Lite** - 간소화된 MBConv + SE 구조\n5. **EfficientNet-B0** (최고 성능 예상) - 원본 B0 아키텍처\n\n치와와와 머핀은 외형이 매우 유사하여 세밀한 특징 추출이 필요"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 다운로드\n",
    "\n",
    "Kaggle API 필요: `pip install kaggle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/chihuahua_muffin'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d samuelcortinhas/muffin-vs-chihuahua-image-classification -p {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.join(DATA_DIR, 'muffin-vs-chihuahua-image-classification.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "print(\"압축 해제 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChihuahuaMuffinDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name, class_idx in [('chihuahua', 0), ('muffin', 1)]:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "train_dataset = ChihuahuaMuffinDataset(train_dir)\n",
    "test_dataset = ChihuahuaMuffinDataset(test_dir, transform=test_transform)\n",
    "\n",
    "print(f\"훈련: {len(train_dataset)}, 테스트: {len(test_dataset)}\")\n",
    "print(f\"클래스 분포: chihuahua={sum(train_dataset.labels==0)}, muffin={sum(train_dataset.labels==1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 시각화 - 치와와와 머핀의 유사성 확인\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "temp_ds = ChihuahuaMuffinDataset(train_dir, transform=test_transform)\n",
    "\n",
    "chi_idx = np.where(temp_ds.labels == 0)[0][:5]\n",
    "muf_idx = np.where(temp_ds.labels == 1)[0][:5]\n",
    "\n",
    "for i, idx in enumerate(chi_idx):\n",
    "    img, _ = temp_ds[idx]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    axes[0, i].imshow(img.permute(1,2,0).clip(0,1))\n",
    "    axes[0, i].set_title('Chihuahua')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, idx in enumerate(muf_idx):\n",
    "    img, _ = temp_ds[idx]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    axes[1, i].imshow(img.permute(1,2,0).clip(0,1))\n",
    "    axes[1, i].set_title('Muffin')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. 모델 정의\n\n### 망 구조 표기법\n- **C**: Convolution (합성곱) - 공간적 특징 추출\n- **DWC**: Depthwise Separable Conv - 채널별 독립 합성곱\n- **BN**: Batch Normalization - 학습 안정화\n- **R**: ReLU / **T**: Tanh / **Swish**: x·σ(x) - 활성화 함수\n- **AvgP** / **MaxP**: Average/Max Pooling\n- **GAP**: Global Average Pooling\n- **D**: Dropout - 정규화\n- **FC**: Fully Connected\n- **Res**: Residual Connection\n- **SE**: Squeeze-and-Excitation - 채널 attention\n- **MBConv**: Mobile Inverted Bottleneck Conv\n\n---\n\n### 4.1 LeNet-5 (베이스라인, LeCun et al., 1998)\n\n**구조**: `C(3,6,5×5) → T → AvgP → C(6,16,5×5) → T → AvgP → C(16,32,5×5) → T → AvgP → Flat → FC → T → FC → T → FC`\n\n**특징**:\n- 최초의 성공적인 CNN (수업시간에 배운 모델)\n- 5×5 큰 커널로 특징 추출\n- Tanh 활성화 함수 사용\n\n**효과/한계**:\n- ✅ CNN의 기본 원리 적용\n- ❌ 얕은 구조로 세밀한 특징 추출 한계\n- ❌ 치와와 눈과 머핀 초콜릿칩 구분 어려움"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class LeNet5(nn.Module):\n    \"\"\"\n    LeNet-5 (LeCun et al., 1998) - 128x128 RGB 버전\n    구조: C(5x5) → T → AvgP → C(5x5) → T → AvgP → C(5x5) → T → AvgP → Flat → FC → T → FC → T → FC\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(3, 6, 5),                # C(3,6,5x5): 128 → 124\n            nn.Tanh(),                          # T\n            nn.AvgPool2d(2, 2),                 # AvgP: 124 → 62\n            nn.Conv2d(6, 16, 5),                # C(6,16,5x5): 62 → 58\n            nn.Tanh(),                          # T\n            nn.AvgPool2d(2, 2),                 # AvgP: 58 → 29\n            nn.Conv2d(16, 32, 5),               # C(16,32,5x5): 29 → 25\n            nn.Tanh(),                          # T\n            nn.AvgPool2d(2, 2),                 # AvgP: 25 → 12\n            nn.Flatten(),                       # Flat: (B,32,12,12) → (B,4608)\n            nn.Linear(32 * 12 * 12, 120),       # FC\n            nn.Tanh(),                          # T\n            nn.Linear(120, 84),                 # FC\n            nn.Tanh(),                          # T\n            nn.Linear(84, num_classes)          # FC: 출력\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nprint(\"LeNet-5: C(5x5) → T → AvgP → C(5x5) → T → AvgP → C(5x5) → T → AvgP → Flat → FC×3\")\nprint(LeNet5())\nprint(f\"파라미터: {sum(p.numel() for p in LeNet5().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 VGGNet-style (Simonyan & Zisserman, 2014)\n\n**구조**: `[C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×3 → MaxP → [C(3×3) → BN → R]×3 → GAP → D → FC`\n\n**특징**:\n- 작은 3×3 커널을 깊게 쌓음 (VGG의 핵심)\n- 3×3 두 번 = 5×5 수용 영역, 더 적은 파라미터\n- BN + ReLU로 현대화\n\n**효과**:\n- ✅ 깊은 네트워크로 세밀한 패턴 학습\n- ✅ 작은 커널로 파라미터 효율성\n- ⚠️ Skip connection 없어 gradient 문제 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class VGGBlock(nn.Module):\n    \"\"\"VGG Block: [C(3x3) → BN → R] × n_convs\"\"\"\n    def __init__(self, in_ch, out_ch, n_convs=2):\n        super().__init__()\n        layers = []\n        for i in range(n_convs):\n            layers.extend([\n                nn.Conv2d(in_ch if i == 0 else out_ch, out_ch, 3, 1, 1),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU()\n            ])\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nclass VGGNet(nn.Module):\n    \"\"\"\n    VGGNet-style - 128x128 RGB\n    구조: VGGBlock×4 + MaxPool×4 + GAP + FC\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            VGGBlock(3, 32, n_convs=2),         # Block1: 128→64\n            nn.MaxPool2d(2, 2),\n            VGGBlock(32, 64, n_convs=2),        # Block2: 64→32\n            nn.MaxPool2d(2, 2),\n            VGGBlock(64, 128, n_convs=3),       # Block3: 32→16\n            nn.MaxPool2d(2, 2),\n            VGGBlock(128, 256, n_convs=3),      # Block4: 16→8\n            nn.MaxPool2d(2, 2),\n            nn.AdaptiveAvgPool2d(1),            # GAP\n            nn.Dropout(0.5)\n        ])\n        self.fc = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"VGGNet: [C(3x3)→BN→R]×n → MaxP (4 blocks) → GAP → D → FC\")\nprint(VGGNet())\nprint(f\"파라미터: {sum(p.numel() for p in VGGNet().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 ResNetCNN (He et al., 2015)\n\n**구조**: `C(3,32,7×7,s2) → BN → R → MaxP → Res(32,64) → Res(64,128) → Res(128,256) → GAP → D(0.5) → FC(256,2)`\n\n**ResBlock 내부**: `C(3×3) → BN → R → C(3×3) → BN → (+shortcut) → R`\n\n**특징**:\n- Residual Connection (skip connection)\n- H(x) = F(x) + x로 잔차 학습\n\n**효과**:\n- ✅ Skip connection으로 gradient flow 개선\n- ✅ Gradient vanishing 문제 해결\n- ✅ VGGNet보다 깊게 학습 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ResidualBlock(nn.Module):\n    \"\"\"Residual Block: C → BN → R → C → BN → (+shortcut) → R\"\"\"\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(),\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_ch)\n        ])\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_ch != out_ch:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n                nn.BatchNorm2d(out_ch)\n            )\n    \n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer(out)\n        out += self.shortcut(x)\n        return nn.ReLU()(out)\n\n\nclass ResNetCNN(nn.Module):\n    \"\"\"\n    ResNetCNN - 128x128 RGB\n    구조: C(7x7) → BN → R → MaxP → Res×3 → GAP → D → FC\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(3, 32, 7, 2, 3, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2, 1),\n            ResidualBlock(32, 64, 1),\n            ResidualBlock(64, 128, 2),\n            ResidualBlock(128, 256, 2),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Dropout(0.5)\n        ])\n        self.fc = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"ResNetCNN: C(7x7) → BN → R → MaxP → Res×3 → GAP → D → FC\")\nprint(ResNetCNN())\nprint(f\"파라미터: {sum(p.numel() for p in ResNetCNN().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 EfficientNet-Lite (간소화된 EfficientNet)\n\n**구조**: `C(3,32,3×3,s2) → BN → Swish → [MBConv]×8 → GAP → D → FC`\n\n**MBConv (Mobile Inverted Bottleneck Conv)**:\n`C(1×1,expand) → BN → Swish → DWC(3×3) → BN → Swish → SE → C(1×1,project) → BN → (+shortcut)`\n\n**특징**:\n- EfficientNet의 핵심 구성요소만 사용한 간소화 버전\n- MBConv + SE + Swish 조합\n- 128×128 입력에 맞게 채널 수 축소\n\n**효과**:\n- ✅ 파라미터 효율적 (원본 B0보다 가벼움)\n- ✅ 빠른 학습 가능\n- ⚠️ 원본 B0 대비 표현력 제한",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class Swish(nn.Module):\n    \"\"\"Swish 활성화: x * sigmoid(x), ReLU보다 smooth\"\"\"\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation Block - 채널 attention\"\"\"\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(channels, channels // reduction),\n            Swish(),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        ])\n    \n    def forward(self, x):\n        b, c, _, _ = x.size()\n        scale = x\n        for layer in self.layers:\n            scale = layer(scale)\n        return x * scale.view(b, c, 1, 1)\n\n\nclass MBConv(nn.Module):\n    \"\"\"\n    Mobile Inverted Bottleneck Conv (EfficientNet의 핵심)\n    구조: C(1x1,expand) → BN → Swish → DWC(kxk) → BN → Swish → SE → C(1x1,project) → BN → (+shortcut)\n    \n    Args:\n        in_ch: 입력 채널\n        out_ch: 출력 채널\n        kernel_size: Depthwise Conv 커널 크기 (3 or 5)\n        expand_ratio: 채널 확장 비율\n        stride: Depthwise Conv stride\n        use_se: SE Block 사용 여부\n    \"\"\"\n    def __init__(self, in_ch, out_ch, kernel_size=3, expand_ratio=4, stride=1, use_se=True):\n        super().__init__()\n        hidden_ch = in_ch * expand_ratio\n        self.use_residual = (stride == 1 and in_ch == out_ch)\n        padding = (kernel_size - 1) // 2  # same padding\n        \n        layers = []\n        # Expand: 1x1 conv로 채널 확장 (expand_ratio > 1인 경우만)\n        if expand_ratio != 1:\n            layers.extend([\n                nn.Conv2d(in_ch, hidden_ch, 1, bias=False),\n                nn.BatchNorm2d(hidden_ch),\n                Swish()\n            ])\n        else:\n            hidden_ch = in_ch\n        \n        # Depthwise: 채널별 독립 kxk conv\n        layers.extend([\n            nn.Conv2d(hidden_ch, hidden_ch, kernel_size, stride, padding, groups=hidden_ch, bias=False),\n            nn.BatchNorm2d(hidden_ch),\n            Swish()\n        ])\n        \n        # SE Block (선택적)\n        if use_se:\n            layers.append(SEBlock(hidden_ch))\n        \n        # Project: 1x1 conv로 채널 축소\n        layers.extend([\n            nn.Conv2d(hidden_ch, out_ch, 1, bias=False),\n            nn.BatchNorm2d(out_ch)\n        ])\n        \n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer(out)\n        if self.use_residual:\n            out = out + x\n        return out\n\n\nclass EfficientNetLite(nn.Module):\n    \"\"\"\n    EfficientNet-Lite - 간소화된 EfficientNet (128x128 RGB)\n    구조: C(3x3,s2) → BN → Swish → MBConv×8 → GAP → D → FC\n    \n    원본 B0보다 가벼운 커스텀 구조\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            # Stem: 128 → 64\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            Swish(),\n            # Stage 1: 64 → 32\n            MBConv(32, 32, kernel_size=3, expand_ratio=1, stride=1),\n            MBConv(32, 48, kernel_size=3, expand_ratio=4, stride=2),\n            # Stage 2: 32 → 16\n            MBConv(48, 48, kernel_size=3, expand_ratio=4, stride=1),\n            MBConv(48, 96, kernel_size=3, expand_ratio=4, stride=2),\n            # Stage 3: 16 → 8\n            MBConv(96, 96, kernel_size=3, expand_ratio=4, stride=1),\n            MBConv(96, 192, kernel_size=3, expand_ratio=4, stride=2),\n            # Stage 4: 8 → 8\n            MBConv(192, 192, kernel_size=3, expand_ratio=4, stride=1),\n            MBConv(192, 256, kernel_size=3, expand_ratio=4, stride=1),\n            # Head\n            nn.AdaptiveAvgPool2d(1),\n            nn.Dropout(0.3)\n        ])\n        self.fc = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"EfficientNet-Lite: C(3x3,s2) → BN → Swish → MBConv×8 → GAP → D → FC\")\nprint(f\"파라미터: {sum(p.numel() for p in EfficientNetLite().parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 EfficientNet-B0 (Tan & Le, 2019) - 최고 성능 예상\n\n**원본 논문의 B0 아키텍처** (입력: 224×224, 여기서는 128×128 적용)\n\n| Stage | Operator | Resolution | Channels | Layers |\n|-------|----------|------------|----------|--------|\n| 1 | MBConv1, k3×3 | 112×112 | 16 | 1 |\n| 2 | MBConv6, k3×3 | 56×56 | 24 | 2 |\n| 3 | MBConv6, k5×5 | 28×28 | 40 | 2 |\n| 4 | MBConv6, k3×3 | 14×14 | 80 | 3 |\n| 5 | MBConv6, k5×5 | 14×14 | 112 | 3 |\n| 6 | MBConv6, k5×5 | 7×7 | 192 | 4 |\n| 7 | MBConv6, k3×3 | 7×7 | 320 | 1 |\n\n**구조 특징**:\n- **MBConv1**: expand_ratio=1 (채널 확장 없음)\n- **MBConv6**: expand_ratio=6 (채널 6배 확장 후 축소)\n- **k3×3 / k5×5**: Depthwise Conv 커널 크기\n- **Compound Scaling**: 깊이, 너비, 해상도 균형있게 스케일링\n\n**효과**:\n- ✅ 원본 논문의 검증된 아키텍처\n- ✅ 5×5 커널로 더 넓은 수용 영역\n- ✅ 단계적 채널 증가로 세밀한 특징 학습\n- ✅ ImageNet에서 검증된 구조",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class EfficientNetB0(nn.Module):\n    \"\"\"\n    EfficientNet-B0 (Tan & Le, 2019) - 원본 논문 아키텍처\n    \n    입력: 128×128 RGB (원본은 224×224)\n    \n    Stage 구성 (원본 B0):\n    - Stage 1: MBConv1, k3×3, 16ch, 1 layer\n    - Stage 2: MBConv6, k3×3, 24ch, 2 layers, stride 2\n    - Stage 3: MBConv6, k5×5, 40ch, 2 layers, stride 2\n    - Stage 4: MBConv6, k3×3, 80ch, 3 layers, stride 2\n    - Stage 5: MBConv6, k5×5, 112ch, 3 layers\n    - Stage 6: MBConv6, k5×5, 192ch, 4 layers, stride 2\n    - Stage 7: MBConv6, k3×3, 320ch, 1 layer\n    - Head: Conv1×1 → 1280ch → GAP → Dropout → FC\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        \n        # B0 Configuration: (expand_ratio, channels, layers, stride, kernel_size)\n        # 원본 논문의 정확한 구성\n        b0_config = [\n            # Stage 1: MBConv1, k3, 16ch, 1 layer\n            (1, 16, 1, 1, 3),\n            # Stage 2: MBConv6, k3, 24ch, 2 layers, stride 2\n            (6, 24, 2, 2, 3),\n            # Stage 3: MBConv6, k5, 40ch, 2 layers, stride 2\n            (6, 40, 2, 2, 5),\n            # Stage 4: MBConv6, k3, 80ch, 3 layers, stride 2\n            (6, 80, 3, 2, 3),\n            # Stage 5: MBConv6, k5, 112ch, 3 layers\n            (6, 112, 3, 1, 5),\n            # Stage 6: MBConv6, k5, 192ch, 4 layers, stride 2\n            (6, 192, 4, 2, 5),\n            # Stage 7: MBConv6, k3, 320ch, 1 layer\n            (6, 320, 1, 1, 3),\n        ]\n        \n        layers = []\n        \n        # Stem: Conv3×3, stride 2, 32ch\n        # 128 → 64\n        layers.extend([\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            Swish()\n        ])\n        \n        # MBConv Stages\n        in_ch = 32\n        for expand_ratio, out_ch, n_layers, stride, kernel_size in b0_config:\n            for i in range(n_layers):\n                # stride는 첫 번째 레이어에만 적용\n                s = stride if i == 0 else 1\n                layers.append(\n                    MBConv(in_ch, out_ch, kernel_size=kernel_size, \n                           expand_ratio=expand_ratio, stride=s, use_se=True)\n                )\n                in_ch = out_ch\n        \n        # Head: Conv1×1 → 1280ch → GAP → Dropout\n        layers.extend([\n            nn.Conv2d(320, 1280, 1, bias=False),\n            nn.BatchNorm2d(1280),\n            Swish(),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Dropout(0.2)\n        ])\n        \n        self.layers = nn.ModuleList(layers)\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"EfficientNet-B0: Stem → MBConv×16 → Conv1×1(1280) → GAP → D → FC\")\nprint(f\"파라미터: {sum(p.numel() for p in EfficientNetB0().parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(images), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images.to(device))\n",
    "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "    return accuracy_score(labels_list, preds), f1_score(labels_list, preds, average='micro')\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, _ = evaluate(model, val_loader)\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 5:\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. StratifiedKFold 5겹 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        image = Image.open(self.base.image_paths[i]).convert('RGB')\n",
    "        return self.transform(image), self.base.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(model_class, model_name, n_splits=5):\n",
    "    results = {'fold_acc': [], 'fold_f1': [], 'test_acc': [], 'test_f1': []}\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    all_indices = np.arange(len(train_dataset))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(all_indices, train_dataset.labels)):\n",
    "        set_seed(42 + fold)\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        print(f\"  훈련: {len(train_idx)}, 검증: {len(val_idx)}\")\n",
    "        \n",
    "        train_subset = TransformDataset(train_dataset, train_idx, train_transform)\n",
    "        val_subset = TransformDataset(train_dataset, val_idx, test_transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32)\n",
    "        \n",
    "        model = model_class().to(device)\n",
    "        model = train_model(model, train_loader, val_loader, epochs=30)\n",
    "        \n",
    "        fold_acc, fold_f1 = evaluate(model, val_loader)\n",
    "        test_acc, test_f1 = evaluate(model, test_loader)\n",
    "        \n",
    "        results['fold_acc'].append(fold_acc)\n",
    "        results['fold_f1'].append(fold_f1)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        results['test_f1'].append(test_f1)\n",
    "        \n",
    "        print(f\"  Val Acc: {fold_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lenet_results = run_kfold(LeNet5, \"LeNet-5 (베이스라인)\")"
  },
  {
   "cell_type": "code",
   "source": "vgg_results = run_kfold(VGGNet, \"VGGNet-style\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lite_results = run_kfold(EfficientNetLite, \"EfficientNet-Lite\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "resnet_results = run_kfold(ResNetCNN, \"ResNetCNN\")"
  },
  {
   "cell_type": "code",
   "source": "b0_results = run_kfold(EfficientNetB0, \"EfficientNet-B0 (최고 성능)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def print_kfold_summary(results, name):\n    print(f\"\\n{name}\")\n    print(\"-\"*40)\n    print(f\"{'Fold':<8} {'Test Acc':<12} {'Test F1':<12}\")\n    for i in range(5):\n        print(f\"{i+1:<8} {results['test_acc'][i]:<12.4f} {results['test_f1'][i]:<12.4f}\")\n    print(\"-\"*40)\n    acc_mean, acc_std = np.mean(results['test_acc']), np.std(results['test_acc'])\n    f1_mean, f1_std = np.mean(results['test_f1']), np.std(results['test_f1'])\n    print(f\"평균     {acc_mean:.4f} ± {acc_std:.4f}  {f1_mean:.4f} ± {f1_std:.4f}\")\n    return acc_mean, acc_std, f1_mean, f1_std\n\nprint(\"=\"*50)\nprint(\"최종 결과 (테스트 데이터)\")\nprint(\"=\"*50)\nlenet_stats = print_kfold_summary(lenet_results, \"LeNet-5 (베이스라인)\")\nvgg_stats = print_kfold_summary(vgg_results, \"VGGNet-style\")\nresnet_stats = print_kfold_summary(resnet_results, \"ResNetCNN\")\nlite_stats = print_kfold_summary(lite_results, \"EfficientNet-Lite\")\nb0_stats = print_kfold_summary(b0_results, \"EfficientNet-B0 (최고 성능)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 시각화 - 5개 모델 비교\nfig, ax = plt.subplots(figsize=(14, 5))\nx = np.arange(5)\nwidth = 0.15\n\nax.bar(x - 2*width, lenet_results['test_acc'], width, label='LeNet-5', color='#ff9999')\nax.bar(x - width, vgg_results['test_acc'], width, label='VGGNet', color='#66b3ff')\nax.bar(x, resnet_results['test_acc'], width, label='ResNetCNN', color='#99ff99')\nax.bar(x + width, lite_results['test_acc'], width, label='EfficientNet-Lite', color='#ffcc99')\nax.bar(x + 2*width, b0_results['test_acc'], width, label='EfficientNet-B0', color='#ff99ff')\n\nax.set_xlabel('Fold')\nax.set_ylabel('Test Accuracy')\nax.set_title('Chihuahua vs Muffin - 5겹 교차검증 결과 (5개 모델 비교)')\nax.set_xticks(x)\nax.set_xticklabels([f'Fold {i+1}' for i in x])\nax.legend(loc='lower right')\nax.set_ylim([0.5, 1.0])\nplt.tight_layout()\nplt.show()\n\n# 평균 성능 비교\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\nmodels = ['LeNet-5\\n(베이스라인)', 'VGGNet', 'ResNetCNN', 'EfficientNet\\n-Lite', 'EfficientNet\\n-B0 (최고)']\nall_results = [lenet_results, vgg_results, resnet_results, lite_results, b0_results]\n\nacc_means = [np.mean(r['test_acc']) for r in all_results]\nacc_stds = [np.std(r['test_acc']) for r in all_results]\nf1_means = [np.mean(r['test_f1']) for r in all_results]\nf1_stds = [np.std(r['test_f1']) for r in all_results]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99ff']\n\naxes[0].bar(models, acc_means, yerr=acc_stds, capsize=5, color=colors)\naxes[0].set_title('평균 Test Accuracy')\naxes[0].set_ylim([0.5, 1.0])\naxes[0].tick_params(axis='x', rotation=0)\n\naxes[1].bar(models, f1_means, yerr=f1_stds, capsize=5, color=colors)\naxes[1].set_title('평균 Test F1 Score')\naxes[1].set_ylim([0.5, 1.0])\naxes[1].tick_params(axis='x', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\n# 파라미터 수 비교\nprint(\"\\n\" + \"=\"*50)\nprint(\"모델별 파라미터 수\")\nprint(\"=\"*50)\nmodels_params = [\n    (\"LeNet-5\", LeNet5()),\n    (\"VGGNet\", VGGNet()),\n    (\"ResNetCNN\", ResNetCNN()),\n    (\"EfficientNet-Lite\", EfficientNetLite()),\n    (\"EfficientNet-B0\", EfficientNetB0())\n]\nfor name, model in models_params:\n    params = sum(p.numel() for p in model.parameters())\n    print(f\"{name:20s}: {params:>10,} 파라미터\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 결론\n\n### 모델별 성능 비교\n\n| 모델 | 구조 | 파라미터 | Test Accuracy | Test F1 Score |\n|------|------|---------|--------------|---------------|\n| LeNet-5 (베이스라인) | C(5×5)→T→AvgP ×3 | ~560K | - ± - | - ± - |\n| VGGNet-style | [C(3×3)→BN→R]×n→MaxP | ~1.9M | - ± - | - ± - |\n| ResNetCNN | ResBlock + Skip | ~600K | - ± - | - ± - |\n| EfficientNet-Lite | MBConv(k3) + SE | ~1.2M | - ± - | - ± - |\n| EfficientNet-B0 (최고) | MBConv(k3,k5) + SE | ~4.0M | - ± - | - ± - |\n\n### 모델 분석\n\n**1. LeNet-5 (베이스라인)**\n- 수업시간에 배운 고전적 CNN 구조\n- 5×5 커널과 Tanh 활성화 사용\n- 얕은 구조로 치와와 눈과 머핀 초콜릿칩의 세밀한 차이 구분 한계\n\n**2. VGGNet-style**\n- 작은 3×3 커널을 깊게 쌓아 수용 영역 확보\n- BN + ReLU로 학습 안정화\n- LeNet-5보다 더 세밀한 특징 추출 가능\n\n**3. ResNetCNN**\n- Residual Connection으로 gradient flow 개선\n- 더 깊은 네트워크 학습 가능\n- Skip connection이 세밀한 특징 보존\n\n**4. EfficientNet-Lite**\n- 간소화된 MBConv 구조 (3×3 커널만 사용)\n- SE Block으로 채널 attention\n- 원본 B0보다 가볍고 빠른 학습\n\n**5. EfficientNet-B0 (최고 성능 예상)**\n- 원본 논문의 검증된 아키텍처\n- 3×3과 5×5 커널 혼합 사용 → 다양한 수용 영역\n- MBConv6 (expand_ratio=6)로 충분한 표현력\n- 16개 MBConv 블록 + 1280ch Head\n- ImageNet에서 검증된 구조\n\n### EfficientNet-Lite vs B0 비교\n\n| 비교 항목 | EfficientNet-Lite | EfficientNet-B0 |\n|----------|------------------|-----------------|\n| MBConv 블록 수 | 8개 | 16개 |\n| 커널 크기 | 3×3만 | 3×3 + 5×5 혼합 |\n| expand_ratio | 1~4 | 1~6 |\n| Head 채널 | 256 | 1280 |\n| 파라미터 | ~1.2M | ~4.0M |\n| 예상 성능 | 중상 | 최고 |\n\n### 결론\n치와와와 머핀은 둥근 형태, 갈색 톤, 점 패턴이 유사하여 세밀한 특징 추출이 핵심.\n\n**EfficientNet-B0**가 가장 높은 성능을 보일 것으로 예상:\n1. 5×5 커널로 더 넓은 수용 영역 확보\n2. expand_ratio=6으로 충분한 채널 확장\n3. 16개 MBConv 블록으로 깊은 특징 학습\n4. SE Block이 눈, 코, 입 등 치와와 고유의 특징에 attention\n\n**EfficientNet-Lite**는 B0 대비 파라미터가 1/3 수준이지만, 간소화된 구조로 인해 세밀한 특징 구분에서 한계가 있을 수 있음."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}