{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 문제 2: 치와와 vs 머핀 분류 (5겹 교차검증 × 5회 반복)\n\n## 개요\n- **데이터셋**: Kaggle Muffin vs Chihuahua (128×128 RGB)\n- **검증 방법**: RepeatedStratifiedKFold (5겹 × 5회 = 25회 학습)\n- **평가 척도**: Accuracy, F1 Score (Micro), F1 Score (Macro)\n\n## 검증 방법: RepeatedStratifiedKFold\n\n```\nRepeatedStratifiedKFold(n_splits=5, n_repeats=5)\n\n반복 1: Fold1 → Fold2 → Fold3 → Fold4 → Fold5\n반복 2: Fold1 → Fold2 → Fold3 → Fold4 → Fold5\n반복 3: Fold1 → Fold2 → Fold3 → Fold4 → Fold5\n반복 4: Fold1 → Fold2 → Fold3 → Fold4 → Fold5\n반복 5: Fold1 → Fold2 → Fold3 → Fold4 → Fold5\n\n총 25회 학습 → 평균 ± 표준편차 산출\n```\n\n**장점**:\n- 단순 5겹 교차검증보다 더 안정적인 성능 추정\n- 서로 다른 무작위 분할로 분산 감소\n- sklearn에서 공식 제공하는 반복 교차검증 방법\n\n## 문제의 특성\n치와와와 머핀은 외형이 매우 유사합니다:\n- **둥근 형태**: 치와와의 얼굴과 머핀의 윗면 모두 원형\n- **갈색 톤**: 치와와의 털색과 머핀의 반죽색이 유사\n- **점 패턴**: 치와와의 눈/코와 머핀의 초콜릿칩이 유사\n\n→ 세밀한 특징 추출이 필요한 도전적인 분류 문제\n\n## 구현 모델 (6개)\n\n### 베이스라인 모델\n1. **LeNet-5** (LeCun, 1998) - 최초의 성공적 CNN, 수업시간에 배운 모델\n\n### 고급 모델\n2. **VGGNet-style** (Simonyan, 2014) - 작은 3×3 커널을 깊게 쌓은 구조\n3. **ResNetCNN** (He, 2015) - Residual Connection으로 깊은 학습\n4. **EfficientNet-Lite** - 간소화된 MBConv + SE 구조\n5. **EfficientNet-B0** (Tan, 2019) - 원본 B0 아키텍처\n6. **ConvNeXt-Tiny** (Liu, 2022) - ViT의 설계 원칙을 CNN에 적용\n\n## 평가 척도 설명\n- **Accuracy**: 전체 정확도 (TP + TN) / Total\n- **F1 (Micro)**: 전체 TP/FP/FN 합산 후 계산 → 균형 데이터에서 Accuracy와 동일\n- **F1 (Macro)**: 클래스별 F1의 평균 → 불균형 데이터에서 소수 클래스 성능 반영"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# -------------------------------\n# 공용 라이브러리 및 유틸 불러오기\n# - torch/torchvision: 모델 정의, 변환, Mixed Precision\n# - sklearn: 교차검증 및 지표 계산\n# - PIL: 이미지 로딩\n# - matplotlib: 시각화 및 한글 폰트 설정\n# - zipfile/os: Kaggle 데이터 압축 해제 및 경로 관리\n# -------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.amp import autocast, GradScaler  # Mixed Precision (새 API)\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import RepeatedStratifiedKFold  # 5겹 × 5회 반복\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport os\nimport zipfile\n\n# 한글 폰트 설정 (Linux 서버 환경) - 그래프 라벨 깨짐 방지\ndef set_korean_font():\n    \"\"\"시스템에서 사용 가능한 한글 폰트를 자동 설정\"\"\"\n    font_candidates = [\n        'NanumGothic', 'NanumBarunGothic', 'Malgun Gothic',\n        'AppleGothic', 'DejaVu Sans', 'Noto Sans CJK KR'\n    ]\n    available_fonts = [f.name for f in fm.fontManager.ttflist]\n    \n    for font in font_candidates:\n        if font in available_fonts:\n            plt.rcParams['font.family'] = font\n            plt.rcParams['axes.unicode_minus'] = False\n            print(f\"한글 폰트 설정: {font}\")\n            return\n    \n    print(\"한글 폰트를 찾지 못했습니다. 영문 라벨을 사용합니다.\")\n\nset_korean_font()\n\n# GPU 우선, Mac이면 MPS, 그 외 CPU 선택\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# GPU 정보 출력 (리포트용)\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# H100 GPU 최적화 기본 설정 + 시드 고정 헬퍼\n",
    "# --------------------------------------------\n",
    "BATCH_SIZE = 128          # H100: 80GB 메모리, 배치 크기 증가\n",
    "NUM_WORKERS = 8           # 데이터 로딩 병렬화\n",
    "PIN_MEMORY = True         # GPU 메모리 전송 최적화\n",
    "USE_AMP = True            # Mixed Precision (BF16/FP16)\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"torch/np 시드를 모두 고정하여 StratifiedKFold 일관성 유지\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# cuDNN 최적화 (고정 입력 크기에서 성능 향상)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"H100 최적화 설정:\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Workers: {NUM_WORKERS}\")\n",
    "print(f\"  - Pin Memory: {PIN_MEMORY}\")\n",
    "print(f\"  - Mixed Precision (AMP): {USE_AMP}\")\n",
    "print(f\"  - cuDNN Benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 다운로드\n",
    "\n",
    "Kaggle API 필요: `pip install kaggle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Kaggle 데이터 다운로드 디렉토리 준비\n",
    "# --------------------------------------------\n",
    "DATA_DIR = './data/chihuahua_muffin'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Kaggle 데이터셋 다운로드 (치와와 vs 머핀)\n",
    "# --------------------------------------------\n",
    "!kaggle datasets download -d samuelcortinhas/muffin-vs-chihuahua-image-classification -p {DATA_DIR}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 다운로드한 zip 압축 해제\n",
    "# --------------------------------------------\n",
    "zip_path = os.path.join(DATA_DIR, 'muffin-vs-chihuahua-image-classification.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "print(\"압축 해제 완료\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Custom Dataset 정의\n",
    "# - 폴더 구조: chihuahua/, muffin/\n",
    "# - 이미지 경로와 라벨을 미리 스캔하여 리스트로 보관\n",
    "# --------------------------------------------\n",
    "class ChihuahuaMuffinDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name, class_idx in [('chihuahua', 0), ('muffin', 1)]:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # PIL로 로드 후 RGB 변환, transform 적용\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 데이터 변환 정의\n",
    "# - train: 색상/밝기 증강 포함 (클래스 유사도 대응)\n",
    "# - test: 증강 없이 정규화만 적용\n",
    "# --------------------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 데이터셋 로드 (train/test 분리 제공)\n",
    "# --------------------------------------------\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "train_dataset = ChihuahuaMuffinDataset(train_dir)\n",
    "test_dataset = ChihuahuaMuffinDataset(test_dir, transform=test_transform)\n",
    "\n",
    "print(f\"훈련: {len(train_dataset)}, 테스트: {len(test_dataset)}\")\n",
    "print(f\"클래스 분포: chihuahua={sum(train_dataset.labels==0)}, muffin={sum(train_dataset.labels==1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 샘플 시각화 - 치와와 vs 머핀의 유사성 확인\n",
    "# --------------------------------------------\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "temp_ds = ChihuahuaMuffinDataset(train_dir, transform=test_transform)\n",
    "\n",
    "chi_idx = np.where(temp_ds.labels == 0)[0][:5]\n",
    "muf_idx = np.where(temp_ds.labels == 1)[0][:5]\n",
    "\n",
    "for i, idx in enumerate(chi_idx):\n",
    "    img, _ = temp_ds[idx]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    axes[0, i].imshow(img.permute(1,2,0).clip(0,1))\n",
    "    axes[0, i].set_title('Chihuahua')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, idx in enumerate(muf_idx):\n",
    "    img, _ = temp_ds[idx]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    axes[1, i].imshow(img.permute(1,2,0).clip(0,1))\n",
    "    axes[1, i].set_title('Muffin')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. 모델 정의\n\n### 망 구조 표기법\n- **C**: Convolution (합성곱) - 공간적 특징 추출\n- **DWC**: Depthwise Separable Conv - 채널별 독립 합성곱\n- **BN**: Batch Normalization - 학습 안정화\n- **R**: ReLU / **T**: Tanh / **Swish**: x·σ(x) - 활성화 함수\n- **AvgP** / **MaxP**: Average/Max Pooling\n- **GAP**: Global Average Pooling\n- **D**: Dropout - 정규화\n- **FC**: Fully Connected\n- **Res**: Residual Connection\n- **SE**: Squeeze-and-Excitation - 채널 attention\n- **MBConv**: Mobile Inverted Bottleneck Conv\n\n---\n\n### 4.1 LeNet-5 (베이스라인, LeCun et al., 1998)\n\n**구조**: `C(3,6,5×5) → T → AvgP → C(6,16,5×5) → T → AvgP → C(16,32,5×5) → T → AvgP → Flat → FC → T → FC → T → FC`\n\n**특징**:\n- 최초의 성공적인 CNN (수업시간에 배운 모델)\n- 5×5 큰 커널로 특징 추출\n- Tanh 활성화 함수 사용\n\n**효과/한계**:\n- ✅ CNN의 기본 원리 적용\n- ❌ 얕은 구조로 세밀한 특징 추출 한계\n- ❌ 치와와 눈과 머핀 초콜릿칩 구분 어려움"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 (LeCun et al., 1998) - 128x128 RGB 버전\n",
    "    구조: C(5x5) → T → AvgP → C(5x5) → T → AvgP → C(5x5) → T → AvgP → Flat → FC → T → FC → T → FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        # 레이어를 ModuleList에 저장해 구조를 한눈에 파악\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3, 6, 5),                # C(3,6,5x5): 128 → 124\n",
    "            nn.Tanh(),                          # T\n",
    "            nn.AvgPool2d(2, 2),                 # AvgP: 124 → 62\n",
    "            nn.Conv2d(6, 16, 5),                # C(6,16,5x5): 62 → 58\n",
    "            nn.Tanh(),                          # T\n",
    "            nn.AvgPool2d(2, 2),                 # AvgP: 58 → 29\n",
    "            nn.Conv2d(16, 32, 5),               # C(16,32,5x5): 29 → 25\n",
    "            nn.Tanh(),                          # T\n",
    "            nn.AvgPool2d(2, 2),                 # AvgP: 25 → 12\n",
    "            nn.Flatten(),                       # Flat: (B,32,12,12) → (B,4608)\n",
    "            nn.Linear(32 * 12 * 12, 120),       # FC\n",
    "            nn.Tanh(),                          # T\n",
    "            nn.Linear(120, 84),                 # FC\n",
    "            nn.Tanh(),                          # T\n",
    "            nn.Linear(84, num_classes)          # FC: 출력\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"LeNet-5: C(5x5) → T → AvgP → C(5x5) → T → AvgP → C(5x5) → T → AvgP → Flat → FC×3\")\n",
    "print(LeNet5())\n",
    "print(f\"파라미터: {sum(p.numel() for p in LeNet5().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 VGGNet-style (Simonyan & Zisserman, 2014)\n\n**구조**: `[C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×3 → MaxP → [C(3×3) → BN → R]×3 → GAP → D → FC`\n\n**특징**:\n- 작은 3×3 커널을 깊게 쌓음 (VGG의 핵심)\n- 3×3 두 번 = 5×5 수용 영역, 더 적은 파라미터\n- BN + ReLU로 현대화\n\n**효과**:\n- ✅ 깊은 네트워크로 세밀한 패턴 학습\n- ✅ 작은 커널로 파라미터 효율성\n- ⚠️ Skip connection 없어 gradient 문제 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    \"\"\"VGG Block: [C(3x3) → BN → R] × n_convs\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, n_convs=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_convs):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_ch if i == 0 else out_ch, out_ch, 3, 1, 1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    VGGNet-style - 128x128 RGB\n",
    "    구조: VGGBlock×4 + MaxPool×4 + GAP + FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            VGGBlock(3, 32, n_convs=2),         # Block1: 128→64\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            VGGBlock(32, 64, n_convs=2),        # Block2: 64→32\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            VGGBlock(64, 128, n_convs=3),       # Block3: 32→16\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            VGGBlock(128, 256, n_convs=3),      # Block4: 16→8\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.AdaptiveAvgPool2d(1),            # GAP\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"VGGNet: [C(3x3)→BN→R]×n → MaxP (4 blocks) → GAP → D → FC\")\n",
    "print(VGGNet())\n",
    "print(f\"파라미터: {sum(p.numel() for p in VGGNet().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 ResNetCNN (He et al., 2015)\n\n**구조**: `C(3,32,7×7,s2) → BN → R → MaxP → Res(32,64) → Res(64,128) → Res(128,256) → GAP → D(0.5) → FC(256,2)`\n\n**ResBlock 내부**: `C(3×3) → BN → R → C(3×3) → BN → (+shortcut) → R`\n\n**특징**:\n- Residual Connection (skip connection)\n- H(x) = F(x) + x로 잔차 학습\n\n**효과**:\n- ✅ Skip connection으로 gradient flow 개선\n- ✅ Gradient vanishing 문제 해결\n- ✅ VGGNet보다 깊게 학습 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block: C → BN → R → C → BN → (+shortcut) → R\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        ])\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out += self.shortcut(x)\n",
    "        return nn.ReLU()(out)\n",
    "\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNetCNN - 128x128 RGB\n",
    "    구조: C(7x7) → BN → R → MaxP → Res×3 → GAP → D → FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3, 32, 7, 2, 3, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1),\n",
    "            ResidualBlock(32, 64, 1),\n",
    "            ResidualBlock(64, 128, 2),\n",
    "            ResidualBlock(128, 256, 2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"ResNetCNN: C(7x7) → BN → R → MaxP → Res×3 → GAP → D → FC\")\n",
    "print(ResNetCNN())\n",
    "print(f\"파라미터: {sum(p.numel() for p in ResNetCNN().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 EfficientNet-Lite (간소화된 EfficientNet)\n\n**구조**: `C(3,32,3×3,s2) → BN → Swish → [MBConv]×8 → GAP → D → FC`\n\n**MBConv (Mobile Inverted Bottleneck Conv)**:\n`C(1×1,expand) → BN → Swish → DWC(3×3) → BN → Swish → SE → C(1×1,project) → BN → (+shortcut)`\n\n**특징**:\n- EfficientNet의 핵심 구성요소만 사용한 간소화 버전\n- MBConv + SE + Swish 조합\n- 128×128 입력에 맞게 채널 수 축소\n\n**효과**:\n- ✅ 파라미터 효율적 (원본 B0보다 가벼움)\n- ✅ 빠른 학습 가능\n- ⚠️ 원본 B0 대비 표현력 제한",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish 활성화: x * sigmoid(x), ReLU보다 smooth\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block - 채널 attention\"\"\"\n",
    "    def __init__(self, channels, reduction=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            Swish(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        scale = x\n",
    "        for layer in self.layers:\n",
    "            scale = layer(scale)\n",
    "        return x * scale.view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Bottleneck Conv (EfficientNet의 핵심)\n",
    "    구조: C(1x1,expand) → BN → Swish → DWC(kxk) → BN → Swish → SE → C(1x1,project) → BN → (+shortcut)\n",
    "    \n",
    "    Args:\n",
    "        in_ch: 입력 채널\n",
    "        out_ch: 출력 채널\n",
    "        kernel_size: Depthwise Conv 커널 크기 (3 or 5)\n",
    "        expand_ratio: 채널 확장 비율\n",
    "        stride: Depthwise Conv stride\n",
    "        use_se: SE Block 사용 여부\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, expand_ratio=4, stride=1, use_se=True):\n",
    "        super().__init__()\n",
    "        hidden_ch = in_ch * expand_ratio\n",
    "        self.use_residual = (stride == 1 and in_ch == out_ch)\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        layers = []\n",
    "        # Expand 단계: 채널 확장\n",
    "        if expand_ratio != 1:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_ch, hidden_ch, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_ch),\n",
    "                Swish()\n",
    "            ])\n",
    "        else:\n",
    "            hidden_ch = in_ch\n",
    "        \n",
    "        # Depthwise Conv: 채널별 독립 필터\n",
    "        layers.extend([\n",
    "            nn.Conv2d(hidden_ch, hidden_ch, kernel_size, stride, padding, groups=hidden_ch, bias=False),\n",
    "            nn.BatchNorm2d(hidden_ch),\n",
    "            Swish()\n",
    "        ])\n",
    "        \n",
    "        # SE Block (선택)\n",
    "        if use_se:\n",
    "            layers.append(SEBlock(hidden_ch))\n",
    "        \n",
    "        # Project 단계: 출력 채널로 축소\n",
    "        layers.extend([\n",
    "            nn.Conv2d(hidden_ch, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        ])\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        if self.use_residual:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class EfficientNetLite(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet-Lite - 간소화된 EfficientNet (128x128 RGB)\n",
    "    구조: C(3x3,s2) → BN → Swish → MBConv×8 → GAP → D → FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Swish(),\n",
    "            MBConv(32, 32, kernel_size=3, expand_ratio=1, stride=1),\n",
    "            MBConv(32, 48, kernel_size=3, expand_ratio=4, stride=2),\n",
    "            MBConv(48, 48, kernel_size=3, expand_ratio=4, stride=1),\n",
    "            MBConv(48, 96, kernel_size=3, expand_ratio=4, stride=2),\n",
    "            MBConv(96, 96, kernel_size=3, expand_ratio=4, stride=1),\n",
    "            MBConv(96, 192, kernel_size=3, expand_ratio=4, stride=2),\n",
    "            MBConv(192, 192, kernel_size=3, expand_ratio=4, stride=1),\n",
    "            MBConv(192, 256, kernel_size=3, expand_ratio=4, stride=1),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.3)\n",
    "        ])\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"EfficientNet-Lite: C(3x3,s2) → BN → Swish → MBConv×8 → GAP → D → FC\")\n",
    "print(f\"파라미터: {sum(p.numel() for p in EfficientNetLite().parameters()):,}\")\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 EfficientNet-B0 (Tan & Le, 2019) - 최고 성능 예상\n\n**원본 논문의 B0 아키텍처** (입력: 224×224, 여기서는 128×128 적용)\n\n| Stage | Operator | Resolution | Channels | Layers |\n|-------|----------|------------|----------|--------|\n| 1 | MBConv1, k3×3 | 112×112 | 16 | 1 |\n| 2 | MBConv6, k3×3 | 56×56 | 24 | 2 |\n| 3 | MBConv6, k5×5 | 28×28 | 40 | 2 |\n| 4 | MBConv6, k3×3 | 14×14 | 80 | 3 |\n| 5 | MBConv6, k5×5 | 14×14 | 112 | 3 |\n| 6 | MBConv6, k5×5 | 7×7 | 192 | 4 |\n| 7 | MBConv6, k3×3 | 7×7 | 320 | 1 |\n\n**구조 특징**:\n- **MBConv1**: expand_ratio=1 (채널 확장 없음)\n- **MBConv6**: expand_ratio=6 (채널 6배 확장 후 축소)\n- **k3×3 / k5×5**: Depthwise Conv 커널 크기\n- **Compound Scaling**: 깊이, 너비, 해상도 균형있게 스케일링\n\n**효과**:\n- ✅ 원본 논문의 검증된 아키텍처\n- ✅ 5×5 커널로 더 넓은 수용 영역\n- ✅ 단계적 채널 증가로 세밀한 특징 학습\n- ✅ ImageNet에서 검증된 구조",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class EfficientNetB0(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet-B0 (Tan & Le, 2019) - 원본 논문 아키텍처\n",
    "\n",
    "    입력: 128×128 RGB (원본은 224×224)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # B0 Configuration: (expand_ratio, channels, layers, stride, kernel_size)\n",
    "        b0_config = [\n",
    "            (1, 16, 1, 1, 3),\n",
    "            (6, 24, 2, 2, 3),\n",
    "            (6, 40, 2, 2, 5),\n",
    "            (6, 80, 3, 2, 3),\n",
    "            (6, 112, 3, 1, 5),\n",
    "            (6, 192, 4, 2, 5),\n",
    "            (6, 320, 1, 1, 3),\n",
    "        ]\n",
    "        \n",
    "        layers = []\n",
    "        # Stem: Conv3×3, stride 2, 32ch -> 128→64\n",
    "        layers.extend([\n",
    "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Swish()\n",
    "        ])\n",
    "        \n",
    "        in_ch = 32\n",
    "        for expand_ratio, out_ch, n_layers, stride, kernel_size in b0_config:\n",
    "            for i in range(n_layers):\n",
    "                s = stride if i == 0 else 1\n",
    "                layers.append(\n",
    "                    MBConv(in_ch, out_ch, kernel_size=kernel_size, \n",
    "                           expand_ratio=expand_ratio, stride=s, use_se=True)\n",
    "                )\n",
    "                in_ch = out_ch\n",
    "        \n",
    "        layers.extend([\n",
    "            nn.Conv2d(320, 1280, 1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            Swish(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.2)\n",
    "        ])\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"EfficientNet-B0: Stem → MBConv×16 → Conv1×1(1280) → GAP → D → FC\")\n",
    "print(f\"파라미터: {sum(p.numel() for p in EfficientNetB0().parameters()):,}\")\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.6 ConvNeXt-Tiny (Liu et al., 2022) - 최신 고성능 CNN\n\n**핵심 아이디어**: Vision Transformer(ViT)의 설계 원칙을 순수 CNN에 적용\n\n**ViT에서 가져온 설계 요소**:\n| ViT 특성 | ConvNeXt 적용 |\n|----------|--------------|\n| 큰 패치 (16×16) | Patchify stem: 4×4 conv, stride 4 |\n| LayerNorm | BatchNorm → LayerNorm 교체 |\n| 넓은 MLP (4× 확장) | 1×1 conv로 4배 채널 확장 |\n| GELU 활성화 | ReLU → GELU 교체 |\n| 적은 정규화 | 활성화 함수 1개만 사용 |\n\n**ConvNeXt Block 구조**:\n```\nDWConv(7×7) → LayerNorm → PWConv(1×1, 4×확장) → GELU → PWConv(1×1, 축소) → LayerScale → + Residual\n```\n\n**주요 특징**:\n1. **Depthwise Conv 7×7**: 채널별 독립 합성곱으로 넓은 수용 영역\n2. **LayerNorm (채널 방향)**: BatchNorm 대신 사용, 배치 크기에 독립적\n3. **Inverted Bottleneck**: MobileNet처럼 채널 확장 후 축소\n4. **LayerScale**: 잔차 연결 전 학습 가능한 스케일링 (초기값 1e-6)\n\n**효과**:\n- ✅ Transformer 수준의 성능을 CNN으로 달성\n- ✅ 7×7 Depthwise로 넓은 패턴 인식 (치와와 얼굴 전체 윤곽)\n- ✅ LayerScale로 안정적인 깊은 네트워크 학습\n- ✅ ImageNet에서 ResNet, Swin Transformer와 경쟁",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ConvNeXt-Tiny 스타일 고성능 CNN (128x128 RGB)\n",
    "# - Depthwise 7x7 → LayerNorm → Pointwise 확장/축소 (GELU)\n",
    "# - 4-stage ConvNeXt 변형: {3,3,9,3} 블록 + 다운샘플\n",
    "# --------------------------------------------\n",
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"채널 차원 기준 LayerNorm\"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight.view(1, -1, 1, 1) * x + self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    \"\"\"ConvNeXt Block: DWConv(7x7) → LN → PWConv×2 + GELU → LayerScale → Residual\"\"\"\n",
    "    def __init__(self, dim, layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.norm = LayerNorm2d(dim)\n",
    "        self.pwconv1 = nn.Conv2d(dim, 4 * dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(4 * dim, dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim)) if layer_scale_init_value > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma.view(1, -1, 1, 1) * x\n",
    "        return shortcut + x\n",
    "\n",
    "\n",
    "class ConvNeXtTiny(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt-Tiny 변형 (128x128 입력)\n",
    "    구조: Stem(4x4,s4) → [Block×{3,3,9,3} + Downsample] → GAP → Dropout → FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        depths = [3, 3, 9, 3]\n",
    "        dims = [96, 192, 384, 768]\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, dims[0], kernel_size=4, stride=4),  # 128 → 32\n",
    "                LayerNorm2d(dims[0])\n",
    "            )\n",
    "        ])\n",
    "        for i in range(3):\n",
    "            self.downsample_layers.append(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(dims[i]),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            blocks = [ConvNeXtBlock(dims[i]) for _ in range(depths[i])]\n",
    "            self.stages.append(nn.ModuleList(blocks))\n",
    "\n",
    "        self.norm = LayerNorm2d(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample_layers[0](x)\n",
    "        for i in range(4):\n",
    "            for block in self.stages[i]:\n",
    "                x = block(x)\n",
    "            if i < 3:\n",
    "                x = self.downsample_layers[i+1](x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean([-2, -1])\n",
    "        x = self.dropout(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"ConvNeXt-Tiny: Stem(4x4,s4) → Blocks×{3,3,9,3} → GAP → D → FC\")\n",
    "print(ConvNeXtTiny())\n",
    "print(f\"파라미터: {sum(p.numel() for p in ConvNeXtTiny().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 학습/평가 유틸리티 함수 (H100 최적화)\n",
    "# --------------------------------------------\n",
    "scaler = GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"Mixed Precision 학습 (H100 최적화)\"\"\"\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # 메모리 효율적\n",
    "        \n",
    "        with autocast('cuda', enabled=USE_AMP):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"평가 함수 - Accuracy, F1 (Micro), F1 (Macro) 반환\"\"\"\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            with autocast('cuda', enabled=USE_AMP):\n",
    "                outputs = model(images)\n",
    "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "    \n",
    "    acc = accuracy_score(labels_list, preds)\n",
    "    f1_micro = f1_score(labels_list, preds, average='micro')\n",
    "    f1_macro = f1_score(labels_list, preds, average='macro')\n",
    "    return acc, f1_micro, f1_macro\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    \"\"\"H100 최적화 학습 함수\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, _, _ = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 7:  # H100에서는 여유 있게\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "print(\"H100 최적화 학습 함수 정의 완료\")\n",
    "print(\"  - Mixed Precision (autocast + GradScaler)\")\n",
    "print(\"  - F1 Score: Micro (=Accuracy) + Macro (클래스별 평균)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. RepeatedStratifiedKFold 5겹 × 5회 교차검증\n\n### 검증 방법\n- **RepeatedStratifiedKFold**: sklearn에서 제공하는 반복 계층화 교차검증\n- **n_splits=5**: 5겹 교차검증 (데이터를 5개 폴드로 분할)\n- **n_repeats=5**: 5회 반복 (서로 다른 무작위 시드로 분할)\n- **총 학습 횟수**: 5 × 5 = 25회\n\n### 장점\n1. 단순 5겹 교차검증보다 더 안정적인 성능 추정\n2. 서로 다른 무작위 분할로 분할 편향 감소\n3. 표준편차 계산에 더 많은 샘플 (25개 vs 5개)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 인덱스 기반 부분집합을 transform과 함께 래핑하는 헬퍼\n",
    "# --------------------------------------------\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        image = Image.open(self.base.image_paths[i]).convert('RGB')\n",
    "        return self.transform(image), self.base.labels[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# RepeatedStratifiedKFold 5겹 × 5회 교차검증 실행 함수\n# - n_splits=5, n_repeats=5 → 총 25회 학습\n# - 각 반복마다 train/val 분리 후 테스트 세트는 고정 평가\n# - torch.compile로 추가 최적화 시도\n# --------------------------------------------\ndef run_repeated_kfold(model_class, model_name, n_splits=5, n_repeats=5):\n    \"\"\"\n    H100 최적화 RepeatedStratifiedKFold\n    \n    Args:\n        model_class: 모델 클래스\n        model_name: 모델 이름 (출력용)\n        n_splits: 교차검증 폴드 수 (기본 5)\n        n_repeats: 반복 횟수 (기본 5)\n    \n    Returns:\n        results: 25회 테스트 성능 딕셔너리\n    \"\"\"\n    results = {\n        'test_acc': [], 'test_f1_micro': [], 'test_f1_macro': []\n    }\n    \n    print()\n    print('='*60)\n    print(f'{model_name}')\n    print(f'RepeatedStratifiedKFold: {n_splits}겹 × {n_repeats}회 = {n_splits * n_repeats}회 학습')\n    print('='*60)\n    \n    # RepeatedStratifiedKFold 설정\n    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n    all_indices = np.arange(len(train_dataset))\n    \n    # 테스트 로더 (고정)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY\n    )\n    \n    # 총 반복 횟수\n    total_iterations = n_splits * n_repeats\n    \n    for i, (train_idx, val_idx) in enumerate(rskf.split(all_indices, train_dataset.labels)):\n        repeat_num = i // n_splits + 1  # 현재 반복 번호 (1~5)\n        fold_num = i % n_splits + 1     # 현재 폴드 번호 (1~5)\n        \n        set_seed(42 + i)\n        \n        # 진행 상황 출력\n        print(f\"[{i+1:2d}/{total_iterations}] 반복{repeat_num}-폴드{fold_num} | Train: {len(train_idx)}, Val: {len(val_idx)}\", end=\" → \")\n        \n        train_subset = TransformDataset(train_dataset, train_idx, train_transform)\n        val_subset = TransformDataset(train_dataset, val_idx, test_transform)\n        \n        train_loader = DataLoader(\n            train_subset, \n            batch_size=BATCH_SIZE, \n            shuffle=True,\n            num_workers=NUM_WORKERS,\n            pin_memory=PIN_MEMORY,\n            drop_last=True\n        )\n        val_loader = DataLoader(\n            val_subset, \n            batch_size=BATCH_SIZE,\n            num_workers=NUM_WORKERS,\n            pin_memory=PIN_MEMORY\n        )\n        \n        # 모델 초기화\n        model = model_class().to(device)\n        \n        # PyTorch 2.x: torch.compile 최적화\n        if hasattr(torch, 'compile'):\n            try:\n                model = torch.compile(model, mode='reduce-overhead')\n            except:\n                pass\n        \n        # 학습\n        model = train_model(model, train_loader, val_loader, epochs=30)\n        \n        # 테스트 평가\n        test_acc, test_f1_micro, test_f1_macro = evaluate(model, test_loader)\n        \n        results['test_acc'].append(test_acc)\n        results['test_f1_micro'].append(test_f1_micro)\n        results['test_f1_macro'].append(test_f1_macro)\n        \n        print(f\"Test Acc: {test_acc:.4f}, F1(Micro): {test_f1_micro:.4f}, F1(Macro): {test_f1_macro:.4f}\")\n        \n        # 메모리 정리\n        del model\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    # 최종 요약\n    print()\n    print(f\"{'='*60}\")\n    print(f\"[{model_name}] 최종 결과 (25회 평균)\")\n    print(f\"  Accuracy:   {np.mean(results['test_acc']):.4f} ± {np.std(results['test_acc']):.4f}\")\n    print(f\"  F1 (Micro): {np.mean(results['test_f1_micro']):.4f} ± {np.std(results['test_f1_micro']):.4f}\")\n    print(f\"  F1 (Macro): {np.mean(results['test_f1_macro']):.4f} ± {np.std(results['test_f1_macro']):.4f}\")\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lenet_results = run_repeated_kfold(LeNet5, \"LeNet-5 (베이스라인)\")"
  },
  {
   "cell_type": "code",
   "source": "vgg_results = run_repeated_kfold(VGGNet, \"VGGNet-style\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lite_results = run_repeated_kfold(EfficientNetLite, \"EfficientNet-Lite\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "resnet_results = run_repeated_kfold(ResNetCNN, \"ResNetCNN\")"
  },
  {
   "cell_type": "code",
   "source": "b0_results = run_repeated_kfold(EfficientNetB0, \"EfficientNet-B0 (최고 성능)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "convnext_results = run_repeated_kfold(ConvNeXtTiny, \"ConvNeXt-Tiny (고성능)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 결과 요약 함수 (RepeatedStratifiedKFold용)\n# - 25회 결과를 5회 반복 × 5겹 형태로 정리\n# --------------------------------------------\ndef print_repeated_kfold_summary(results, name, n_splits=5, n_repeats=5):\n    \"\"\"\n    RepeatedStratifiedKFold 결과를 반복별/폴드별로 정리하여 출력\n    \"\"\"\n    print()\n    print(f\"{'='*80}\")\n    print(f\"{name} - RepeatedStratifiedKFold 결과 ({n_splits}겹 × {n_repeats}회)\")\n    print(f\"{'='*80}\")\n    \n    # 반복별 평균 계산\n    print(f\"\\n{'반복별 평균 성능':^40}\")\n    print(\"-\"*70)\n    print(f\"{'반복':<8} {'Accuracy':<18} {'F1(Micro)':<18} {'F1(Macro)':<18}\")\n    print(\"-\"*70)\n    \n    for r in range(n_repeats):\n        start_idx = r * n_splits\n        end_idx = start_idx + n_splits\n        \n        acc_mean = np.mean(results['test_acc'][start_idx:end_idx])\n        f1mi_mean = np.mean(results['test_f1_micro'][start_idx:end_idx])\n        f1ma_mean = np.mean(results['test_f1_macro'][start_idx:end_idx])\n        \n        print(f\"반복 {r+1:<4} {acc_mean:.4f}             {f1mi_mean:.4f}             {f1ma_mean:.4f}\")\n    \n    print(\"-\"*70)\n    \n    # 전체 평균 및 표준편차\n    acc_mean, acc_std = np.mean(results['test_acc']), np.std(results['test_acc'])\n    f1_micro_mean, f1_micro_std = np.mean(results['test_f1_micro']), np.std(results['test_f1_micro'])\n    f1_macro_mean, f1_macro_std = np.mean(results['test_f1_macro']), np.std(results['test_f1_macro'])\n    \n    print(f\"\\n전체 (25회)\")\n    print(f\"  Accuracy:   {acc_mean:.4f} ± {acc_std:.4f}\")\n    print(f\"  F1 (Micro): {f1_micro_mean:.4f} ± {f1_micro_std:.4f}\")\n    print(f\"  F1 (Macro): {f1_macro_mean:.4f} ± {f1_macro_std:.4f}\")\n    \n    return acc_mean, acc_std, f1_micro_mean, f1_micro_std, f1_macro_mean, f1_macro_std\n\nprint(\"=\"*60)\nprint(\"Final Results (RepeatedStratifiedKFold: 5겹 × 5회 = 25회)\")\nprint(\"=\"*60)\nlenet_stats = print_repeated_kfold_summary(lenet_results, \"LeNet-5 (Baseline)\")\nvgg_stats = print_repeated_kfold_summary(vgg_results, \"VGGNet-style\")\nresnet_stats = print_repeated_kfold_summary(resnet_results, \"ResNetCNN\")\nlite_stats = print_repeated_kfold_summary(lite_results, \"EfficientNet-Lite\")\nb0_stats = print_repeated_kfold_summary(b0_results, \"EfficientNet-B0 (Best)\")\nconvnext_stats = print_repeated_kfold_summary(convnext_results, \"ConvNeXt-Tiny (Best)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 시각화 - RepeatedStratifiedKFold 결과 (5겹 × 5회 = 25회)\n# --------------------------------------------\n\n# 한글 폰트 재설정\nimport platform\nif platform.system() == 'Darwin':  # macOS\n    plt.rcParams['font.family'] = 'AppleGothic'\nelif platform.system() == 'Windows':\n    plt.rcParams['font.family'] = 'Malgun Gothic'\nelse:  # Linux\n    font_list = [f.name for f in fm.fontManager.ttflist if 'Nanum' in f.name or 'Gothic' in f.name]\n    if font_list:\n        plt.rcParams['font.family'] = font_list[0]\nplt.rcParams['axes.unicode_minus'] = False\n\n# 모델별 결과 정리\nmodels = ['LeNet-5', 'VGGNet', 'ResNetCNN', 'Eff-Lite', 'Eff-B0', 'ConvNeXt']\nall_results = [lenet_results, vgg_results, resnet_results, lite_results, b0_results, convnext_results]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99ff', '#c2a3ff']\n\n# ============================================\n# 그래프 1: 모든 평가 지표 비교 (25회 평균 ± 표준편차)\n# ============================================\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. Accuracy\nacc_means = [np.mean(r['test_acc']) for r in all_results]\nacc_stds = [np.std(r['test_acc']) for r in all_results]\nbars1 = axes[0].bar(models, acc_means, yerr=acc_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\naxes[0].set_title('Accuracy (정확도)', fontsize=12, fontweight='bold')\naxes[0].set_ylim([0.5, 1.0])\naxes[0].set_ylabel('Score')\naxes[0].tick_params(axis='x', rotation=15)\nfor bar, mean, std in zip(bars1, acc_means, acc_stds):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n                 f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n\n# 2. F1 Score (Micro)\nf1_micro_means = [np.mean(r['test_f1_micro']) for r in all_results]\nf1_micro_stds = [np.std(r['test_f1_micro']) for r in all_results]\nbars2 = axes[1].bar(models, f1_micro_means, yerr=f1_micro_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\naxes[1].set_title('F1 Score (Micro)', fontsize=12, fontweight='bold')\naxes[1].set_ylim([0.5, 1.0])\naxes[1].set_ylabel('Score')\naxes[1].tick_params(axis='x', rotation=15)\nfor bar, mean, std in zip(bars2, f1_micro_means, f1_micro_stds):\n    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n                 f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n\n# 3. F1 Score (Macro)\nf1_macro_means = [np.mean(r['test_f1_macro']) for r in all_results]\nf1_macro_stds = [np.std(r['test_f1_macro']) for r in all_results]\nbars3 = axes[2].bar(models, f1_macro_means, yerr=f1_macro_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\naxes[2].set_title('F1 Score (Macro)', fontsize=12, fontweight='bold')\naxes[2].set_ylim([0.5, 1.0])\naxes[2].set_ylabel('Score')\naxes[2].tick_params(axis='x', rotation=15)\nfor bar, mean, std in zip(bars3, f1_macro_means, f1_macro_stds):\n    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n                 f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n\nplt.suptitle('Chihuahua vs Muffin - 모델별 성능 비교\\n(RepeatedStratifiedKFold: 5겹 × 5회 = 25회)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('task2_metrics_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 그래프 2: 세 지표를 하나의 그래프에 그룹으로 표시\n# ============================================\nfig, ax = plt.subplots(figsize=(16, 6))\n\nx = np.arange(len(models))\nwidth = 0.25\n\nbars_acc = ax.bar(x - width, acc_means, width, yerr=acc_stds, label='Accuracy', color='#4ECDC4', capsize=3, edgecolor='black', linewidth=0.5)\nbars_micro = ax.bar(x, f1_micro_means, width, yerr=f1_micro_stds, label='F1 (Micro)', color='#FF6B6B', capsize=3, edgecolor='black', linewidth=0.5)\nbars_macro = ax.bar(x + width, f1_macro_means, width, yerr=f1_macro_stds, label='F1 (Macro)', color='#45B7D1', capsize=3, edgecolor='black', linewidth=0.5)\n\nax.set_xlabel('모델', fontsize=12)\nax.set_ylabel('Score', fontsize=12)\nax.set_title('Chihuahua vs Muffin - 평가 지표별 모델 성능 비교 (25회 평균)', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(models, fontsize=10)\nax.legend(loc='lower right', fontsize=10)\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\n\nfor bar, mean in zip(bars_acc, acc_means):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.savefig('task2_grouped_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 그래프 3: 반복별 F1 (Macro) 성능 비교 (5회 반복 평균)\n# ============================================\nfig, ax = plt.subplots(figsize=(14, 5))\n\nn_splits = 5\nn_repeats = 5\nx = np.arange(n_repeats)\nwidth = 0.13\n\nfor i, (result, model, color) in enumerate(zip(all_results, models, colors)):\n    # 각 반복별 평균 계산\n    repeat_means = []\n    for r in range(n_repeats):\n        start_idx = r * n_splits\n        end_idx = start_idx + n_splits\n        repeat_means.append(np.mean(result['test_f1_macro'][start_idx:end_idx]))\n    \n    offset = (i - 2.5) * width\n    ax.bar(x + offset, repeat_means, width, label=model, color=color, edgecolor='black', linewidth=0.5)\n\nax.set_xlabel('반복 횟수 (Repeat)', fontsize=12)\nax.set_ylabel('F1 Score (Macro)', fontsize=12)\nax.set_title('Chihuahua vs Muffin - 반복별 F1 (Macro) 성능 (각 반복은 5겹 평균)', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels([f'반복 {i+1}' for i in x], fontsize=10)\nax.legend(loc='lower right', fontsize=9, ncol=2)\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('task2_repeat_f1macro.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 그래프 4: 박스플롯 - 25회 결과 분포\n# ============================================\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# 박스플롯 데이터 준비\nbox_data = [r['test_acc'] for r in all_results]\npositions = np.arange(len(models))\n\nbp = ax.boxplot(box_data, positions=positions, widths=0.6, patch_artist=True)\n\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n    patch.set_alpha(0.7)\n\nax.set_xticks(positions)\nax.set_xticklabels(models, fontsize=10)\nax.set_ylabel('Accuracy', fontsize=12)\nax.set_title('Chihuahua vs Muffin - Accuracy 분포 (25회 결과)', fontsize=14, fontweight='bold')\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('task2_boxplot_accuracy.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 결과 요약 테이블 출력\n# ============================================\nprint(\"\\n\" + \"=\"*90)\nprint(\"📊 Chihuahua vs Muffin - 최종 결과 요약\")\nprint(\"   (RepeatedStratifiedKFold: 5겹 × 5회 = 25회)\")\nprint(\"=\"*90)\nprint(f\"{'모델':<18} {'Accuracy':<20} {'F1 (Micro)':<20} {'F1 (Macro)':<20}\")\nprint(\"-\"*90)\nfor model, result in zip(models, all_results):\n    acc_m, acc_s = np.mean(result['test_acc']), np.std(result['test_acc'])\n    f1mi_m, f1mi_s = np.mean(result['test_f1_micro']), np.std(result['test_f1_micro'])\n    f1ma_m, f1ma_s = np.mean(result['test_f1_macro']), np.std(result['test_f1_macro'])\n    print(f\"{model:<18} {acc_m:.4f} ± {acc_s:.4f}      {f1mi_m:.4f} ± {f1mi_s:.4f}      {f1ma_m:.4f} ± {f1ma_s:.4f}\")\nprint(\"=\"*90)\nprint(\"\\n※ 참고: 균형 데이터셋에서 F1 (Micro) = Accuracy 입니다.\")\nprint(\"※ F1 (Macro)는 각 클래스(치와와/머핀)의 F1을 평균하여 클래스별 성능을 균등하게 반영합니다.\")\nprint(\"※ RepeatedStratifiedKFold는 5겹 교차검증을 5회 반복하여 총 25회 학습합니다.\")\n\n# ============================================\n# 파라미터 수 비교\n# ============================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"📐 모델별 파라미터 수\")\nprint(\"=\"*60)\nmodels_params = [\n    (\"LeNet-5\", LeNet5()),\n    (\"VGGNet\", VGGNet()),\n    (\"ResNetCNN\", ResNetCNN()),\n    (\"EfficientNet-Lite\", EfficientNetLite()),\n    (\"EfficientNet-B0\", EfficientNetB0()),\n    (\"ConvNeXt-Tiny\", ConvNeXtTiny())\n]\nfor name, model in models_params:\n    params = sum(p.numel() for p in model.parameters())\n    print(f\"{name:20s}: {params:>12,} params\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 결론\n\n### 검증 방법: RepeatedStratifiedKFold\n\n```python\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nrskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n# 총 25회 학습 (5겹 × 5회 반복)\n```\n\n**장점**:\n- 단순 5겹 교차검증보다 더 안정적인 성능 추정\n- 서로 다른 무작위 분할로 분할 편향 감소\n- 표준편차 계산에 더 많은 샘플 (25개 vs 5개)\n\n---\n\n### 모델별 성능 비교\n\n| 모델 | 연도 | 핵심 구조 | 파라미터 | 예상 성능 |\n|------|------|----------|---------|----------|\n| LeNet-5 | 1998 | C(5×5)→Tanh→AvgP | ~560K | 낮음 (베이스라인) |\n| VGGNet | 2014 | [C(3×3)→BN→R]×n→MaxP | ~1.9M | 중간 |\n| ResNetCNN | 2015 | Residual Block + Skip | ~600K | 중상 |\n| EfficientNet-Lite | 2019 | MBConv(k3) + SE | ~1.2M | 높음 |\n| EfficientNet-B0 | 2019 | MBConv(k3,k5) + SE | ~4.0M | 최고 |\n| ConvNeXt-Tiny | 2022 | DWConv(7×7) + LN + GELU | ~28M | 최고 |\n\n※ 실제 성능은 위 시각화 결과 참조\n\n---\n\n### 평가 척도 해석\n\n| 척도 | 계산 방식 | 의미 |\n|------|----------|------|\n| **Accuracy** | (TP+TN) / 전체 | 전체 정확도 |\n| **F1 (Micro)** | 전체 TP,FP,FN 합산 | 균형 데이터에서 Accuracy와 동일 |\n| **F1 (Macro)** | 클래스별 F1 평균 | 치와와/머핀 각각의 성능 균등 반영 |\n\n※ 균형 데이터셋에서 F1 (Micro) = Accuracy는 sklearn의 정상 동작입니다.\n\n---\n\n### 모델별 상세 분석\n\n#### 1. LeNet-5 (베이스라인, 1998)\n- 수업시간에 배운 고전적 CNN 구조\n- 5×5 커널과 Tanh 활성화 사용\n- **한계**: 얕은 구조로 치와와 눈과 머핀 초콜릿칩의 세밀한 차이 구분 어려움\n\n#### 2. VGGNet-style (2014)\n- 작은 3×3 커널을 깊게 쌓아 수용 영역 확보\n- BN + ReLU로 학습 안정화\n- **장점**: LeNet-5보다 더 세밀한 특징 추출 가능\n- **한계**: Skip connection 없어 깊은 학습 시 gradient 문제\n\n#### 3. ResNetCNN (2015)\n- Residual Connection으로 gradient flow 개선\n- H(x) = F(x) + x로 잔차 학습\n- **장점**: 더 깊은 네트워크 학습 가능, 세밀한 특징 보존\n\n#### 4. EfficientNet-Lite\n- 간소화된 MBConv 구조 (3×3 커널만 사용)\n- SE Block으로 채널 attention\n- **장점**: 원본 B0보다 가볍고 빠른 학습\n- **한계**: 5×5 커널 미사용으로 수용 영역 제한\n\n#### 5. EfficientNet-B0 (2019)\n- 원본 논문의 검증된 아키텍처\n- 3×3과 5×5 커널 혼합 사용 → 다양한 수용 영역\n- MBConv6 (expand_ratio=6)로 충분한 표현력\n- **장점**: ImageNet에서 검증된 구조, SE Block으로 채널 attention\n\n#### 6. ConvNeXt-Tiny (2022) - 최신 아키텍처\n- Vision Transformer의 설계 원칙을 순수 CNN에 적용\n- **핵심 특징**:\n  - Depthwise Conv 7×7: 가장 넓은 수용 영역\n  - LayerNorm: 배치 크기에 독립적인 정규화\n  - GELU: smooth한 활성화 함수\n  - LayerScale: 안정적인 깊은 네트워크 학습\n- **장점**: Transformer 수준 성능을 CNN으로 달성\n\n---\n\n### CNN 아키텍처 발전 흐름\n\n```\nLeNet-5 (1998)     VGGNet (2014)      ResNet (2015)      EfficientNet (2019)   ConvNeXt (2022)\n     │                  │                  │                    │                   │\n  5×5 커널         3×3 깊게 쌓기      Residual Conn.      MBConv + SE        ViT 원칙 적용\n     │                  │                  │                    │                   │\n  Tanh + AvgP       BN + ReLU          Skip + BN          Swish + SE        LN + GELU + 7×7\n```\n\n---\n\n### 치와와 vs 머핀: 왜 어려운 분류 문제인가?\n\n| 특징 | 치와와 | 머핀 | 구분 난이도 |\n|------|--------|------|------------|\n| 전체 형태 | 둥근 얼굴 | 둥근 윗면 | 매우 어려움 |\n| 색상 | 갈색 털 | 갈색 반죽 | 매우 어려움 |\n| 점 패턴 | 눈, 코 | 초콜릿칩 | 매우 어려움 |\n| 질감 | 부드러운 털 | 거친 표면 | 중간 |\n| 귀/가장자리 | 뾰족한 귀 | 없음 | 쉬움 |\n\n### 핵심 인사이트\n\n**치와와와 머핀을 구분하는 가장 중요한 특징 = 귀 형태**\n\n- 치와와: 뾰족한 귀가 있음\n- 머핀: 귀 형태가 없음\n\n→ **넓은 수용 영역**을 가진 모델이 귀 형태를 인식하는 데 유리\n- EfficientNet-B0: 5×5 커널로 넓은 수용 영역\n- ConvNeXt-Tiny: 7×7 Depthwise로 가장 넓은 수용 영역\n\n---\n\n### 결론\n\n**EfficientNet-B0와 ConvNeXt-Tiny**가 가장 높은 성능을 보일 것으로 예상:\n\n1. **EfficientNet-B0**: \n   - 5×5 커널로 넓은 수용 영역\n   - SE Block으로 채널 attention\n   - ImageNet에서 검증된 구조\n\n2. **ConvNeXt-Tiny**: \n   - 7×7 Depthwise로 가장 넓은 수용 영역\n   - 최신 설계 원칙 (LayerNorm, GELU, LayerScale)\n   - Transformer 수준의 성능을 CNN으로 달성\n\n이 문제에서는 **넓은 수용 영역 + 채널 Attention**이 성능의 핵심입니다.\n\n---\n\n### RepeatedStratifiedKFold의 이점\n\n| 항목 | 단순 5겹 CV | RepeatedStratifiedKFold (5×5) |\n|------|------------|------------------------------|\n| 총 학습 횟수 | 5회 | 25회 |\n| 표준편차 샘플 수 | 5개 | 25개 |\n| 분할 편향 | 한 번의 분할에 의존 | 5번의 다른 분할로 평균 |\n| 결과 안정성 | 낮음 | 높음 |\n| 계산 비용 | 낮음 | 5배 증가 |\n\n→ 더 신뢰할 수 있는 성능 추정을 위해 RepeatedStratifiedKFold 사용"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}