{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 문제 1: 개와 고양이 분류\n\n- **데이터셋**: Kaggle Cats and Dogs (128×128 RGB)\n- **검증**: 반복 홀드아웃 5회, 훈련:검증 = 3:2\n- **평가**: Accuracy, F1 Score (Micro)\n\n## 모델 구조\n1. **LeNet-5** (베이스라인) - 수업시간에 배운 고전적 CNN\n2. **VGGNet-style** - 작은 3×3 커널을 깊게 쌓은 구조\n3. **ResNetCNN** - Residual Connection으로 깊은 학습\n4. **SEResNet** (최고 성능 예상) - SE attention + Residual"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 다운로드\n",
    "\n",
    "Kaggle API 필요: `pip install kaggle`\n",
    "\n",
    "kaggle.json을 ~/.kaggle/에 저장해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 디렉토리\n",
    "DATA_DIR = './data/cats_dogs'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle에서 다운로드\n",
    "!kaggle datasets download -d samuelcortinhas/cats-and-dogs-image-classification -p {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 해제\n",
    "zip_path = os.path.join(DATA_DIR, 'cats-and-dogs-image-classification.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "print(\"압축 해제 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name, class_idx in [('cats', 0), ('dogs', 1)]:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 정의\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "train_dataset = CatsDogsDataset(train_dir)\n",
    "test_dataset = CatsDogsDataset(test_dir, transform=test_transform)\n",
    "\n",
    "print(f\"훈련: {len(train_dataset)}, 테스트: {len(test_dataset)}\")\n",
    "print(f\"클래스 분포: cats={sum(train_dataset.labels==0)}, dogs={sum(train_dataset.labels==1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 시각화\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "temp_ds = CatsDogsDataset(train_dir, transform=test_transform)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = temp_ds[i * 50]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    ax.imshow(img.permute(1,2,0).clip(0,1))\n",
    "    ax.set_title('Cat' if label == 0 else 'Dog')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. 모델 정의\n\n### 망 구조 표기법\n- **C**: Convolution (합성곱) - 공간적 특징 추출\n- **BN**: Batch Normalization - 학습 안정화, 수렴 가속\n- **R**: ReLU / **T**: Tanh - 활성화 함수\n- **AvgP** / **MaxP**: Average/Max Pooling - 다운샘플링\n- **GAP**: Global Average Pooling - 파라미터 감소\n- **D**: Dropout - 정규화, 과적합 방지\n- **FC**: Fully Connected - 분류를 위한 선형 변환\n- **Flat**: Flatten - 2D→1D 변환\n- **Res**: Residual Connection - skip connection\n- **SE**: Squeeze-and-Excitation - 채널별 attention\n\n---\n\n### 4.1 LeNet-5 (베이스라인, LeCun et al., 1998)\n\n**구조**: `C(3,6,5×5) → T → AvgP → C(6,16,5×5) → T → AvgP → Flat → FC → T → FC → T → FC`\n\n**특징**:\n- 최초의 성공적인 CNN (수업시간에 배운 모델)\n- 5×5 큰 커널로 특징 추출\n- RGB 입력에 맞게 변형 (원본은 grayscale)\n\n**효과/한계**:\n- ✅ CNN의 기본 원리 적용\n- ❌ 128×128 큰 이미지에는 얕은 구조\n- ❌ 현대적 기법(BN, ReLU, Dropout) 미적용\n- ❌ 개/고양이 같은 복잡한 분류에 한계"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class LeNet5(nn.Module):\n    \"\"\"\n    LeNet-5 (LeCun et al., 1998) - 128x128 RGB 버전\n    구조: C(3,6,5x5) → T → AvgP → C(6,16,5x5) → T → AvgP → Flat → FC → T → FC → T → FC\n    \n    - 원본 LeNet-5를 RGB 입력에 맞게 변형\n    - 5x5 커널과 Tanh 활성화 (원본 논문)\n    - 128x128 → 62x62 → 31x31 → 13x13 → 6x6 (AvgPool 추가)\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        # 망 구조: C → T → AvgP → C → T → AvgP → C → T → AvgP → Flat → FC → T → FC → T → FC\n        self.layers = nn.ModuleList([\n            # Feature Extraction (128x128 입력 처리를 위해 레이어 추가)\n            nn.Conv2d(3, 6, 5),                # C(3,6,5x5): 128 → 124\n            nn.Tanh(),                          # T: 활성화\n            nn.AvgPool2d(2, 2),                 # AvgP: 124 → 62\n            nn.Conv2d(6, 16, 5),                # C(6,16,5x5): 62 → 58\n            nn.Tanh(),                          # T: 활성화\n            nn.AvgPool2d(2, 2),                 # AvgP: 58 → 29\n            nn.Conv2d(16, 32, 5),               # C(16,32,5x5): 29 → 25\n            nn.Tanh(),                          # T: 활성화\n            nn.AvgPool2d(2, 2),                 # AvgP: 25 → 12\n            nn.Flatten(),                       # Flat: (B,32,12,12) → (B,4608)\n            # Classification\n            nn.Linear(32 * 12 * 12, 120),       # FC(4608,120)\n            nn.Tanh(),                          # T: 활성화\n            nn.Linear(120, 84),                 # FC(120,84)\n            nn.Tanh(),                          # T: 활성화\n            nn.Linear(84, num_classes)          # FC(84,2): 출력층\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nprint(\"LeNet-5: C(5x5) → T → AvgP → C(5x5) → T → AvgP → C(5x5) → T → AvgP → Flat → FC → T → FC → T → FC\")\nprint(LeNet5())\nprint(f\"파라미터: {sum(p.numel() for p in LeNet5().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 VGGNet-style (Simonyan & Zisserman, 2014)\n\n**구조**: `[C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×3 → MaxP → [C(3×3) → BN → R]×3 → GAP → D → FC`\n\n**특징**:\n- 작은 3×3 커널을 깊게 쌓음 (VGG의 핵심 아이디어)\n- 3×3 두 번 = 5×5 수용 영역, 더 적은 파라미터\n- Batch Normalization + ReLU로 현대화\n- MaxPooling으로 점진적 다운샘플링\n\n**효과**:\n- ✅ 깊은 네트워크로 복잡한 패턴 학습\n- ✅ 작은 커널로 파라미터 효율성\n- ✅ BN으로 학습 안정화\n- ⚠️ Skip connection 없어 gradient 문제 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class VGGBlock(nn.Module):\n    \"\"\"\n    VGG Block: 3x3 Conv를 여러 번 쌓은 블록\n    구조: [C(3x3) → BN → R] × n_convs\n    \"\"\"\n    def __init__(self, in_ch, out_ch, n_convs=2):\n        super().__init__()\n        layers = []\n        for i in range(n_convs):\n            layers.extend([\n                nn.Conv2d(in_ch if i == 0 else out_ch, out_ch, 3, 1, 1),  # C(3x3)\n                nn.BatchNorm2d(out_ch),                                   # BN\n                nn.ReLU()                                                  # R\n            ])\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nclass VGGNet(nn.Module):\n    \"\"\"\n    VGGNet-style (Simonyan & Zisserman, 2014) - 128x128 RGB\n    구조: VGGBlock×4 + MaxPool×4 + GAP + FC\n    \n    - 3x3 작은 커널만 사용 (VGG의 핵심)\n    - BN과 ReLU로 현대화\n    - 128x128 → 64 → 32 → 16 → 8 → GAP\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        # 망 구조: VGGBlock → MaxP → VGGBlock → MaxP → ... → GAP → D → FC\n        self.layers = nn.ModuleList([\n            # Block 1: 128x128 → 64x64\n            VGGBlock(3, 32, n_convs=2),            # [C(3x3) → BN → R] × 2\n            nn.MaxPool2d(2, 2),                     # MaxP\n            # Block 2: 64x64 → 32x32\n            VGGBlock(32, 64, n_convs=2),           # [C(3x3) → BN → R] × 2\n            nn.MaxPool2d(2, 2),                     # MaxP\n            # Block 3: 32x32 → 16x16\n            VGGBlock(64, 128, n_convs=3),          # [C(3x3) → BN → R] × 3\n            nn.MaxPool2d(2, 2),                     # MaxP\n            # Block 4: 16x16 → 8x8\n            VGGBlock(128, 256, n_convs=3),         # [C(3x3) → BN → R] × 3\n            nn.MaxPool2d(2, 2),                     # MaxP: 8x8\n            nn.AdaptiveAvgPool2d(1),               # GAP: 8x8 → 1x1\n            nn.Dropout(0.5)                         # D: 과적합 방지\n        ])\n        self.fc = nn.Linear(256, num_classes)      # FC(256,2): 분류\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"VGGNet: [C(3x3)→BN→R]×2 → MaxP → ... (4 blocks) → GAP → D → FC\")\nprint(VGGNet())\nprint(f\"파라미터: {sum(p.numel() for p in VGGNet().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 ResNetCNN (He et al., 2015)\n\n**구조**: `C(3,32,7×7,s2) → BN → R → MaxP → Res(32,64) → Res(64,128) → Res(128,256) → GAP → D(0.5) → FC(256,2)`\n\n**ResBlock 내부**: `C(3×3) → BN → R → C(3×3) → BN → (+shortcut) → R`\n\n**특징**:\n- Residual Connection (skip connection) 도입\n- 7×7 큰 커널로 시작 (넓은 수용 영역)\n- MaxPool로 초기 다운샘플링\n- 3개의 Residual Block\n\n**효과**:\n- ✅ Skip connection으로 gradient flow 개선\n- ✅ 깊은 네트워크에서도 안정적인 학습\n- ✅ VGGNet보다 적은 파라미터로 더 좋은 성능\n- ✅ 개/고양이 같은 복잡한 분류에 효과적"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ResidualBlock(nn.Module):\n    \"\"\"\n    Residual Block (He et al., 2015)\n    구조: C → BN → R → C → BN → (+shortcut) → R\n    \n    핵심 아이디어: H(x) = F(x) + x\n    - 잔차 F(x)를 학습하여 gradient flow 개선\n    \"\"\"\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),  # C: 3x3 conv\n            nn.BatchNorm2d(out_ch),                              # BN\n            nn.ReLU(),                                           # R\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),      # C: 3x3 conv\n            nn.BatchNorm2d(out_ch)                               # BN\n        ])\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_ch != out_ch:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n                nn.BatchNorm2d(out_ch)\n            )\n    \n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer(out)\n        out += self.shortcut(x)  # Residual: F(x) + x\n        return nn.ReLU()(out)\n\n\nclass ResNetCNN(nn.Module):\n    \"\"\"\n    ResNetCNN (He et al., 2015) - 128x128 RGB\n    구조: C(7x7,s2) → BN → R → MaxP → Res(32,64) → Res(64,128) → Res(128,256) → GAP → D → FC\n    \n    - 7x7 큰 커널로 시작 (넓은 수용 영역)\n    - 3개의 Residual Block\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(3, 32, 7, 2, 3, bias=False),   # C(3,32,7x7): 128 → 64\n            nn.BatchNorm2d(32),                      # BN\n            nn.ReLU(),                               # R\n            nn.MaxPool2d(3, 2, 1),                   # MaxP: 64 → 32\n            ResidualBlock(32, 64, 1),                # Res: 32x32\n            ResidualBlock(64, 128, 2),               # Res: 16x16\n            ResidualBlock(128, 256, 2),              # Res: 8x8\n            nn.AdaptiveAvgPool2d(1),                 # GAP: 1x1\n            nn.Dropout(0.5)                          # D\n        ])\n        self.fc = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"ResNetCNN: C(7x7) → BN → R → MaxP → Res → Res → Res → GAP → D → FC\")\nprint(ResNetCNN())\nprint(f\"파라미터: {sum(p.numel() for p in ResNetCNN().parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 SEResNet (Hu et al., 2017) - 최고 성능 예상\n\n**구조**: `C(3,32,7×7,s2) → BN → R → MaxP → SERes(32,64) → SERes(64,128) → SERes(128,256) → GAP → D(0.5) → FC(256,2)`\n\n**SEResBlock 내부**: `C → BN → R → C → BN → SE → (+shortcut) → R`\n\n**SE Block**: `GAP → FC(C,C/16) → R → FC(C/16,C) → Sigmoid → Scale`\n- Squeeze: GAP로 채널별 전역 정보 압축\n- Excitation: FC로 채널 간 관계 학습 후 중요도 계산\n\n**특징**:\n- Squeeze-and-Excitation 메커니즘\n- 채널별 attention으로 털 패턴, 귀 모양 등 중요 특징 강조\n- 수업에서 다루지 않은 최신 구조\n\n**효과**:\n- ✅ 채널별 중요도를 동적으로 조절\n- ✅ 개/고양이 구분에 중요한 특징 강조\n- ✅ 적은 파라미터 추가로 큰 성능 향상\n- ✅ Residual + Attention 시너지",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SEBlock(nn.Module):\n    \"\"\"\n    Squeeze-and-Excitation Block (Hu et al., 2017)\n    구조: GAP → FC(C, C/r) → R → FC(C/r, C) → Sigmoid → Scale\n    \"\"\"\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.AdaptiveAvgPool2d(1),                          # Squeeze\n            nn.Flatten(),\n            nn.Linear(channels, channels // reduction),       # FC: 축소\n            nn.ReLU(),\n            nn.Linear(channels // reduction, channels),       # FC: 복원\n            nn.Sigmoid()\n        ])\n    \n    def forward(self, x):\n        b, c, _, _ = x.size()\n        scale = x\n        for layer in self.layers:\n            scale = layer(scale)\n        return x * scale.view(b, c, 1, 1)\n\n\nclass SEResidualBlock(nn.Module):\n    \"\"\"\n    SE-Residual Block\n    구조: C → BN → R → C → BN → SE → (+shortcut) → R\n    \"\"\"\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(),\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            SEBlock(out_ch)\n        ])\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_ch != out_ch:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n                nn.BatchNorm2d(out_ch)\n            )\n    \n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer(out)\n        out += self.shortcut(x)\n        return nn.ReLU()(out)\n\n\nclass SEResNet(nn.Module):\n    \"\"\"\n    SE-ResNet (Hu et al., 2017) - 128x128 RGB\n    구조: C(7x7) → BN → R → MaxP → SERes×3 → GAP → D → FC\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(3, 32, 7, 2, 3, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2, 1),\n            SEResidualBlock(32, 64, 1),\n            SEResidualBlock(64, 128, 2),\n            SEResidualBlock(128, 256, 2),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Dropout(0.5)\n        ])\n        self.fc = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nprint(\"SEResNet: C(7x7) → BN → R → MaxP → SERes → SERes → SERes → GAP → D → FC\")\nprint(SEResNet())\nprint(f\"파라미터: {sum(p.numel() for p in SEResNet().parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(images), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images.to(device))\n",
    "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "    return accuracy_score(labels_list, preds), f1_score(labels_list, preds, average='micro')\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, _ = evaluate(model, val_loader)\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 5:\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 반복 홀드아웃 검증 (5회)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        image = Image.open(self.base.image_paths[i]).convert('RGB')\n",
    "        return self.transform(image), self.base.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_holdout(model_class, model_name, n_repeats=5):\n",
    "    results = {'accuracy': [], 'f1_score': []}\n",
    "    all_indices = np.arange(len(train_dataset))\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    for i in range(n_repeats):\n",
    "        set_seed(42 + i)\n",
    "        \n",
    "        # 3:2 분할\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            all_indices, test_size=0.4, stratify=train_dataset.labels, random_state=42+i\n",
    "        )\n",
    "        \n",
    "        train_subset = TransformDataset(train_dataset, train_idx, train_transform)\n",
    "        val_subset = TransformDataset(train_dataset, val_idx, test_transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32)\n",
    "        \n",
    "        model = model_class().to(device)\n",
    "        model = train_model(model, train_loader, val_loader, epochs=30)\n",
    "        \n",
    "        acc, f1 = evaluate(model, test_loader)\n",
    "        results['accuracy'].append(acc)\n",
    "        results['f1_score'].append(f1)\n",
    "        print(f\"[{i+1}/{n_repeats}] Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lenet_results = run_holdout(LeNet5, \"LeNet-5 (베이스라인)\")"
  },
  {
   "cell_type": "code",
   "source": "vgg_results = run_holdout(VGGNet, \"VGGNet-style\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_results = run_holdout(ResNetCNN, \"ResNetCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senet_results = run_holdout(SEResNet, \"SEResNet (최고 성능)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def print_summary(results, name):\n    acc_mean, acc_std = np.mean(results['accuracy']), np.std(results['accuracy'])\n    f1_mean, f1_std = np.mean(results['f1_score']), np.std(results['f1_score'])\n    print(f\"{name}\")\n    print(f\"  Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n    return acc_mean, acc_std, f1_mean, f1_std\n\nprint(\"=\"*50)\nprint(\"최종 결과\")\nprint(\"=\"*50)\nlenet_stats = print_summary(lenet_results, \"LeNet-5\")\nvgg_stats = print_summary(vgg_results, \"VGGNet\")\nresnet_stats = print_summary(resnet_results, \"ResNetCNN\")\nsenet_stats = print_summary(senet_results, \"SEResNet\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 시각화\nfig, ax = plt.subplots(figsize=(12, 5))\nx = np.arange(5)\nwidth = 0.2\n\nax.bar(x - 1.5*width, lenet_results['accuracy'], width, label='LeNet-5')\nax.bar(x - 0.5*width, vgg_results['accuracy'], width, label='VGGNet')\nax.bar(x + 0.5*width, resnet_results['accuracy'], width, label='ResNetCNN')\nax.bar(x + 1.5*width, senet_results['accuracy'], width, label='SEResNet')\n\nax.set_xlabel('반복')\nax.set_ylabel('Accuracy')\nax.set_title('Cats vs Dogs - 모델 성능 비교')\nax.set_xticks(x)\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 결론\n\n| 모델 | 구조 특징 | Accuracy | F1 Score |\n|------|----------|----------|----------|\n| LeNet-5 | C(5×5)-AvgP 반복, Tanh | - | - |\n| VGGNet | [C(3×3)]×n-MaxP 반복 | - | - |\n| ResNetCNN | Residual Connection | - | - |\n| SEResNet | SE Attention + Residual | - | - |\n\n### 모델별 분석\n\n- **LeNet-5 (베이스라인)**: 1998년 고전적 구조. 5×5 큰 커널과 Tanh 활성화. 128×128 이미지에는 얕은 구조로 한계.\n\n- **VGGNet-style**: 3×3 작은 커널을 깊게 쌓아 수용 영역 확보. BN과 ReLU로 현대화했으나 skip connection 없음.\n\n- **ResNetCNN**: Residual Connection으로 gradient vanishing 해결. 입력을 출력에 더해 잔차 학습Jean.\n\n- **SEResNet**: SE Block으로 채널별 중요도 동적 조절. 개/고양이 구분에서 털 패턴, 귀 모양 등 중요 특징 강조. 최고 성능 예상."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}