{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 문제 1: 개와 고양이 분류 (Cats vs Dogs Classification)\n\n## 과제 개요\n- **데이터셋**: Kaggle Cats and Dogs (128×128 RGB, 2 클래스)\n- **검증 방법**: 반복 홀드아웃 5회 (Repeated Holdout)\n- **데이터 분할**: 훈련:검증 = 3:2 (60%:40%)\n- **테스트**: 별도 테스트 데이터셋 사용\n- **평가 척도**: Accuracy, F1 Score (Micro), F1 Score (Macro)\n\n---\n\n## 구현 모델 (5개)\n\n| # | 모델 | 연도 | 핵심 특징 | 역할 |\n|---|------|------|----------|------|\n| 1 | **LeNet-5** | 1998 | 5×5 커널, Tanh, AvgPool | 베이스라인 (수업 모델) |\n| 2 | **VGGNet** | 2014 | 3×3 커널 반복, BN+ReLU | 깊은 CNN |\n| 3 | **ResNetCNN** | 2015 | Residual Connection | Skip Connection |\n| 4 | **SEResNet** | 2017 | SE Block (채널 Attention) | Attention 메커니즘 |\n| 5 | **ConvNeXt** | 2022 | Depthwise 7×7, LayerNorm, GELU | 최신 CNN 구조 |\n\n---\n\n## 평가 척도 설명\n\n| 척도 | 계산 방법 | 특징 |\n|------|----------|------|\n| **Accuracy** | 정답 수 / 전체 수 | 전체 정확도 |\n| **F1 (Micro)** | 전체 TP,FP,FN 합산 후 F1 계산 | 균형 데이터에서 = Accuracy |\n| **F1 (Macro)** | 클래스별 F1의 산술 평균 | 클래스별 성능 균등 반영 |\n\n---\n\n## 실험 설계\n\n```\n[훈련 데이터] ──┬── 60% Train ──> 모델 학습\n               └── 40% Val ────> Early Stopping 기준\n                                  \n[테스트 데이터] ──────────────────> 최종 성능 평가\n\n위 과정을 시드를 바꿔가며 5회 반복 → 평균 ± 표준편차 보고\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 공용 라이브러리 및 유틸 불러오기\n",
    "# - torch/torchvision: 모델 정의, 변환, Mixed Precision\n",
    "# - sklearn: 데이터 분할 및 지표 계산\n",
    "# - PIL: 이미지 로딩\n",
    "# - matplotlib: 시각화 및 한글 폰트 설정\n",
    "# - zipfile/os: Kaggle 데이터 압축 해제 및 경로 관리\n",
    "# -------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler  # Mixed Precision (새 API)\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# 한글 폰트 설정 (Linux 서버 환경) - 그래프 라벨 깨짐 방지\n",
    "def set_korean_font():\n",
    "    \"\"\"시스템에서 사용 가능한 한글 폰트 자동 설정\"\"\"\n",
    "    font_candidates = [\n",
    "        'NanumGothic', 'NanumBarunGothic', 'Malgun Gothic',\n",
    "        'AppleGothic', 'DejaVu Sans', 'Noto Sans CJK KR'\n",
    "    ]\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            plt.rcParams['font.family'] = font\n",
    "            plt.rcParams['axes.unicode_minus'] = False\n",
    "            print(f\"한글 폰트 설정: {font}\")\n",
    "            return\n",
    "    \n",
    "    # 폰트를 찾지 못한 경우 영문으로 대체\n",
    "    print(\"한글 폰트를 찾지 못했습니다. 영문 라벨을 사용합니다.\")\n",
    "\n",
    "set_korean_font()\n",
    "\n",
    "# GPU 우선, Mac이면 MPS, 그 외 CPU 선택\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# GPU 정보 출력 (리포트용)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# H100 GPU 최적화 기본 설정 + 시드 고정 헬퍼\n",
    "# - 고정 배치/입력 크기를 가정하여 cuDNN benchmark 활성화\n",
    "# - Mixed Precision 사용 여부는 USE_AMP로 통제\n",
    "# --------------------------------------------\n",
    "BATCH_SIZE = 128          # H100: 80GB 메모리, 배치 크기 증가\n",
    "NUM_WORKERS = 8           # 데이터 로딩 병렬화\n",
    "PIN_MEMORY = True         # GPU 메모리 전송 최적화\n",
    "USE_AMP = True            # Mixed Precision (BF16/FP16)\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"torch/np 시드를 모두 고정하여 반복 홀드아웃 안정화\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# cuDNN 최적화 (고정 입력 크기에서 성능 향상)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"H100 최적화 설정:\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Workers: {NUM_WORKERS}\")\n",
    "print(f\"  - Pin Memory: {PIN_MEMORY}\")\n",
    "print(f\"  - Mixed Precision (AMP): {USE_AMP}\")\n",
    "print(f\"  - cuDNN Benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 다운로드\n",
    "\n",
    "Kaggle API 필요: `pip install kaggle`\n",
    "\n",
    "kaggle.json을 ~/.kaggle/에 저장해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Kaggle 데이터 다운로드를 위한 디렉토리 준비\n",
    "# git에 데이터가 포함되지 않도록 ./data 하위에만 저장\n",
    "# --------------------------------------------\n",
    "DATA_DIR = './data/cats_dogs'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Kaggle 데이터셋 다운로드 (사전 준비: ~/.kaggle/kaggle.json)\n",
    "# --------------------------------------------\n",
    "!kaggle datasets download -d samuelcortinhas/cats-and-dogs-image-classification -p {DATA_DIR}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 다운로드한 zip 압축 해제\n",
    "# train/test 디렉토리가 생성됨\n",
    "# --------------------------------------------\n",
    "zip_path = os.path.join(DATA_DIR, 'cats-and-dogs-image-classification.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "print(\"압축 해제 완료\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Custom Dataset 정의\n",
    "# - 폴더 구조: cats/, dogs/\n",
    "# - 이미지 경로와 라벨을 미리 스캔하여 리스트로 보관\n",
    "# - __getitem__에서 PIL 이미지를 로드 후 transform 적용\n",
    "# --------------------------------------------\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name, class_idx in [('cats', 0), ('dogs', 1)]:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(class_idx)\n",
    "        \n",
    "        # numpy 배열로 변환해 stratify 등에 사용\n",
    "        self.labels = np.array(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # PIL로 로드 후 RGB 고정, transform 적용\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 데이터 변환 정의\n",
    "# - train: 기본 리사이즈 후 수평 뒤집기/회전으로 데이터 증강\n",
    "# - test: 검증/테스트는 증강 없이 정규화만 적용\n",
    "# --------------------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 데이터셋 로드 (train/test 분리 제공)\n",
    "# --------------------------------------------\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "train_dataset = CatsDogsDataset(train_dir)\n",
    "test_dataset = CatsDogsDataset(test_dir, transform=test_transform)\n",
    "\n",
    "print(f\"훈련: {len(train_dataset)}, 테스트: {len(test_dataset)}\")\n",
    "print(f\"클래스 분포: cats={sum(train_dataset.labels==0)}, dogs={sum(train_dataset.labels==1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 샘플 시각화\n",
    "# - 무작위 위치 대신 일정 간격으로 샘플 선택 (i*50)\n",
    "# - 정규화 해제 후 RGB로 표시\n",
    "# --------------------------------------------\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "temp_ds = CatsDogsDataset(train_dir, transform=test_transform)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = temp_ds[i * 50]\n",
    "    # 정규화 해제: (x * std) + mean\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    ax.imshow(img.permute(1,2,0).clip(0,1))\n",
    "    ax.set_title('Cat' if label == 0 else 'Dog')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. 모델 정의\n\n### 망 구조 표기법\n- **C**: Convolution (합성곱) - 공간적 특징 추출\n- **BN**: Batch Normalization - 학습 안정화, 수렴 가속\n- **R**: ReLU / **T**: Tanh - 활성화 함수\n- **AvgP** / **MaxP**: Average/Max Pooling - 다운샘플링\n- **GAP**: Global Average Pooling - 파라미터 감소\n- **D**: Dropout - 정규화, 과적합 방지\n- **FC**: Fully Connected - 분류를 위한 선형 변환\n- **Flat**: Flatten - 2D→1D 변환\n- **Res**: Residual Connection - skip connection\n- **SE**: Squeeze-and-Excitation - 채널별 attention\n\n---\n\n### 4.1 LeNet-5 (베이스라인, LeCun et al., 1998)\n\n**구조**: `C(3,6,5×5) → T → AvgP → C(6,16,5×5) → T → AvgP → Flat → FC → T → FC → T → FC`\n\n**특징**:\n- 최초의 성공적인 CNN (수업시간에 배운 모델)\n- 5×5 큰 커널로 특징 추출\n- RGB 입력에 맞게 변형 (원본은 grayscale)\n\n**효과/한계**:\n- ✅ CNN의 기본 원리 적용\n- ❌ 128×128 큰 이미지에는 얕은 구조\n- ❌ 현대적 기법(BN, ReLU, Dropout) 미적용\n- ❌ 개/고양이 같은 복잡한 분류에 한계"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 (LeCun et al., 1998) - 128x128 RGB 버전\n",
    "    구조: C(3,6,5x5) → T → AvgP → C(6,16,5x5) → T → AvgP → C(16,32,5x5) → T → AvgP → Flat → FC → T → FC → T → FC\n",
    "    \n",
    "    - 원본 LeNet-5를 RGB 입력에 맞게 변형\n",
    "    - 5x5 커널과 Tanh 활성화 (원본 논문)\n",
    "    - 128x128 → 62x62 → 31x31 → 13x13 → 6x6 (AvgPool 추가)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        # 망 구조를 ModuleList로 노출하여 각 레이어를 쉽게 확인\n",
    "        self.layers = nn.ModuleList([\n",
    "            # Feature Extraction (128x128 입력 처리)\n",
    "            nn.Conv2d(3, 6, 5),                # C(3,6,5x5): 128 → 124\n",
    "            nn.Tanh(),                          # T: 활성화\n",
    "            nn.AvgPool2d(2, 2),                 # AvgP: 124 → 62\n",
    "            nn.Conv2d(6, 16, 5),                # C(6,16,5x5): 62 → 58\n",
    "            nn.Tanh(),                          # T: 활성화\n",
    "            nn.AvgPool2d(2, 2),                 # AvgP: 58 → 29\n",
    "            nn.Conv2d(16, 32, 5),               # C(16,32,5x5): 29 → 25\n",
    "            nn.Tanh(),                          # T: 활성화\n",
    "            nn.AvgPool2d(2, 2),                 # AvgP: 25 → 12\n",
    "            nn.Flatten(),                       # Flat: (B,32,12,12) → (B,4608)\n",
    "            # Classification\n",
    "            nn.Linear(32 * 12 * 12, 120),       # FC(4608,120)\n",
    "            nn.Tanh(),                          # T: 활성화\n",
    "            nn.Linear(120, 84),                 # FC(120,84)\n",
    "            nn.Tanh(),                          # T: 활성화\n",
    "            nn.Linear(84, num_classes)          # FC(84,2): 출력층\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ModuleList 순회로 순전파 진행\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"LeNet-5: C(5x5) → T → AvgP → C(5x5) → T → AvgP → C(5x5) → T → AvgP → Flat → FC → T → FC → T → FC\")\n",
    "print(LeNet5())\n",
    "print(f\"파라미터: {sum(p.numel() for p in LeNet5().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 VGGNet-style (Simonyan & Zisserman, 2014)\n\n**구조**: `[C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×2 → MaxP → [C(3×3) → BN → R]×3 → MaxP → [C(3×3) → BN → R]×3 → GAP → D → FC`\n\n**특징**:\n- 작은 3×3 커널을 깊게 쌓음 (VGG의 핵심 아이디어)\n- 3×3 두 번 = 5×5 수용 영역, 더 적은 파라미터\n- Batch Normalization + ReLU로 현대화\n- MaxPooling으로 점진적 다운샘플링\n\n**효과**:\n- ✅ 깊은 네트워크로 복잡한 패턴 학습\n- ✅ 작은 커널로 파라미터 효율성\n- ✅ BN으로 학습 안정화\n- ⚠️ Skip connection 없어 gradient 문제 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG Block: 3x3 Conv를 여러 번 쌓은 블록\n",
    "    구조: [C(3x3) → BN → R] × n_convs\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, n_convs=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_convs):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_ch if i == 0 else out_ch, out_ch, 3, 1, 1),  # C(3x3)\n",
    "                nn.BatchNorm2d(out_ch),                                   # BN\n",
    "                nn.ReLU()                                                  # R\n",
    "            ])\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    VGGNet-style (Simonyan & Zisserman, 2014) - 128x128 RGB\n",
    "    구조: VGGBlock×4 + MaxPool×4 + GAP + FC\n",
    "    \n",
    "    - 3x3 작은 커널만 사용 (VGG의 핵심)\n",
    "    - BN과 ReLU로 현대화\n",
    "    - 128x128 → 64 → 32 → 16 → 8 → GAP\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        # 망 구조: VGGBlock → MaxP → VGGBlock → MaxP → ... → GAP → D → FC\n",
    "        self.layers = nn.ModuleList([\n",
    "            # Block 1: 128x128 → 64x64\n",
    "            VGGBlock(3, 32, n_convs=2),            # [C(3x3) → BN → R] × 2\n",
    "            nn.MaxPool2d(2, 2),                     # MaxP\n",
    "            # Block 2: 64x64 → 32x32\n",
    "            VGGBlock(32, 64, n_convs=2),           # [C(3x3) → BN → R] × 2\n",
    "            nn.MaxPool2d(2, 2),                     # MaxP\n",
    "            # Block 3: 32x32 → 16x16\n",
    "            VGGBlock(64, 128, n_convs=3),          # [C(3x3) → BN → R] × 3\n",
    "            nn.MaxPool2d(2, 2),                     # MaxP\n",
    "            # Block 4: 16x16 → 8x8\n",
    "            VGGBlock(128, 256, n_convs=3),         # [C(3x3) → BN → R] × 3\n",
    "            nn.MaxPool2d(2, 2),                     # MaxP: 8x8\n",
    "            nn.AdaptiveAvgPool2d(1),               # GAP: 8x8 → 1x1\n",
    "            nn.Dropout(0.5)                         # D: 과적합 방지\n",
    "        ])\n",
    "        self.fc = nn.Linear(256, num_classes)      # FC(256,2): 분류\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"VGGNet: [C(3x3)→BN→R]×2 → MaxP → ... (4 blocks) → GAP → D → FC\")\n",
    "print(VGGNet())\n",
    "print(f\"파라미터: {sum(p.numel() for p in VGGNet().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 ResNetCNN (He et al., 2015)\n\n**구조**: `C(3,32,7×7,s2) → BN → R → MaxP → Res(32,64) → Res(64,128) → Res(128,256) → GAP → D(0.5) → FC(256,2)`\n\n**ResBlock 내부**: `C(3×3) → BN → R → C(3×3) → BN → (+shortcut) → R`\n\n**특징**:\n- Residual Connection (skip connection) 도입\n- 7×7 큰 커널로 시작 (넓은 수용 영역)\n- MaxPool로 초기 다운샘플링\n- 3개의 Residual Block\n\n**효과**:\n- ✅ Skip connection으로 gradient flow 개선\n- ✅ 깊은 네트워크에서도 안정적인 학습\n- ✅ VGGNet보다 적은 파라미터로 더 좋은 성능\n- ✅ 개/고양이 같은 복잡한 분류에 효과적"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block (He et al., 2015)\n",
    "    구조: C → BN → R → C → BN → (+shortcut) → R\n",
    "    \n",
    "    핵심 아이디어: H(x) = F(x) + x\n",
    "    - 잔차 F(x)를 학습하여 gradient flow 개선\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),  # C: 3x3 conv\n",
    "            nn.BatchNorm2d(out_ch),                              # BN\n",
    "            nn.ReLU(),                                           # R\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),      # C: 3x3 conv\n",
    "            nn.BatchNorm2d(out_ch)                               # BN\n",
    "        ])\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out += self.shortcut(x)  # Residual: F(x) + x\n",
    "        return nn.ReLU()(out)\n",
    "\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNetCNN (He et al., 2015) - 128x128 RGB\n",
    "    구조: C(7x7,s2) → BN → R → MaxP → Res(32,64) → Res(64,128) → Res(128,256) → GAP → D → FC\n",
    "    \n",
    "    - 7x7 큰 커널로 시작 (넓은 수용 영역)\n",
    "    - 3개의 Residual Block\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3, 32, 7, 2, 3, bias=False),   # C(3,32,7x7): 128 → 64\n",
    "            nn.BatchNorm2d(32),                      # BN\n",
    "            nn.ReLU(),                               # R\n",
    "            nn.MaxPool2d(3, 2, 1),                   # MaxP: 64 → 32\n",
    "            ResidualBlock(32, 64, 1),                # Res: 32x32\n",
    "            ResidualBlock(64, 128, 2),               # Res: 16x16\n",
    "            ResidualBlock(128, 256, 2),              # Res: 8x8\n",
    "            nn.AdaptiveAvgPool2d(1),                 # GAP: 1x1\n",
    "            nn.Dropout(0.5)                          # D\n",
    "        ])\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"ResNetCNN: C(7x7) → BN → R → MaxP → Res → Res → Res → GAP → D → FC\")\n",
    "print(ResNetCNN())\n",
    "print(f\"파라미터: {sum(p.numel() for p in ResNetCNN().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 SEResNet (Hu et al., 2017) - 최고 성능 예상\n\n**구조**: `C(3,32,7×7,s2) → BN → R → MaxP → SERes(32,64) → SERes(64,128) → SERes(128,256) → GAP → D(0.5) → FC(256,2)`\n\n**SEResBlock 내부**: `C → BN → R → C → BN → SE → (+shortcut) → R`\n\n**SE Block**: `GAP → FC(C,C/16) → R → FC(C/16,C) → Sigmoid → Scale`\n- Squeeze: GAP로 채널별 전역 정보 압축\n- Excitation: FC로 채널 간 관계 학습 후 중요도 계산\n\n**특징**:\n- Squeeze-and-Excitation 메커니즘\n- 채널별 attention으로 털 패턴, 귀 모양 등 중요 특징 강조\n- 수업에서 다루지 않은 최신 구조\n\n**효과**:\n- ✅ 채널별 중요도를 동적으로 조절\n- ✅ 개/고양이 구분에 중요한 특징 강조\n- ✅ 적은 파라미터 추가로 큰 성능 향상\n- ✅ Residual + Attention 시너지",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block (Hu et al., 2017)\n",
    "    구조: GAP → FC(C, C/r) → R → FC(C/r, C) → Sigmoid → Scale\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.AdaptiveAvgPool2d(1),                          # Squeeze\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, channels // reduction),       # FC: 축소\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),       # FC: 복원\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        scale = x\n",
    "        for layer in self.layers:\n",
    "            scale = layer(scale)\n",
    "        return x * scale.view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class SEResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SE-Residual Block\n",
    "    구조: C → BN → R → C → BN → SE → (+shortcut) → R\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            SEBlock(out_ch)\n",
    "        ])\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out += self.shortcut(x)\n",
    "        return nn.ReLU()(out)\n",
    "\n",
    "\n",
    "class SEResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SE-ResNet (Hu et al., 2017) - 128x128 RGB\n",
    "    구조: C(7x7) → BN → R → MaxP → SERes×3 → GAP → D → FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(3, 32, 7, 2, 3, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1),\n",
    "            SEResidualBlock(32, 64, 1),\n",
    "            SEResidualBlock(64, 128, 2),\n",
    "            SEResidualBlock(128, 256, 2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.5)\n",
    "        ])\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"SEResNet: C(7x7) → BN → R → MaxP → SERes → SERes → SERes → GAP → D → FC\")\n",
    "print(SEResNet())\n",
    "print(f\"파라미터: {sum(p.numel() for p in SEResNet().parameters()):,}\")\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 ConvNeXt-Tiny (Liu et al., 2022) - 최신 CNN 구조\n\n**배경**: Vision Transformer(ViT)의 성공 이후, CNN도 Transformer의 설계 원칙을 도입하여 성능을 끌어올릴 수 있음을 보여준 모델\n\n**구조**: `Stem(4×4,s4) → [ConvNeXtBlock × {3,3,9,3}] → GAP → D → FC`\n\n**ConvNeXtBlock 내부**:\n```\nDWConv(7×7) → LayerNorm → PWConv(×4) → GELU → PWConv → LayerScale → (+Residual)\n```\n\n**핵심 설계 원칙 (ViT에서 차용)**:\n1. **Depthwise Conv 7×7**: Transformer의 큰 수용영역 모방 (ViT: 16×16 패치)\n2. **LayerNorm**: BatchNorm 대신 사용 (Transformer 스타일)\n3. **GELU 활성화**: ReLU 대신 GELU (Transformer와 동일)\n4. **Inverted Bottleneck**: 채널을 4배 확장 후 축소 (MobileNet/ViT MLP)\n5. **LayerScale**: 학습 안정화를 위한 학습 가능한 스케일 파라미터\n6. **Patchify Stem**: 4×4 stride 4 conv로 초기 다운샘플링 (ViT 패치 임베딩과 유사)\n\n**효과**:\n- ✅ ViT 수준의 성능을 CNN으로 달성\n- ✅ Transformer 대비 학습 효율성 (더 적은 데이터/연산)\n- ✅ 수업에서 다루지 않은 2022년 최신 구조\n- ✅ ImageNet에서 ResNet, ViT를 능가하는 성능",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ConvNeXt-Tiny 스타일 고성능 CNN (128x128 RGB)\n",
    "# - Depthwise 7x7 → LayerNorm → Pointwise 확장/축소 (GELU)\n",
    "# - Stage별 다운샘플을 포함한 4-stage ConvNeXt-Tiny 변형\n",
    "# --------------------------------------------\n",
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"채널 차원 기준 LayerNorm (ConvNeXt에서 사용)\"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight.view(1, -1, 1, 1) * x + self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt Block\n",
    "    구조: DWConv(7x7) → LN → PWConv(×4) → GELU → PWConv → LayerScale → Residual\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.norm = LayerNorm2d(dim)\n",
    "        self.pwconv1 = nn.Conv2d(dim, 4 * dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(4 * dim, dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim)) if layer_scale_init_value > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma.view(1, -1, 1, 1) * x\n",
    "        return shortcut + x\n",
    "\n",
    "\n",
    "class ConvNeXtTiny(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt-Tiny 변형 (이미지넷 대비 축소, 128x128 입력)\n",
    "    구조: Stem(4x4,s4) → [Block×{3,3,9,3} + Downsample] → GAP → Dropout → FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        depths = [3, 3, 9, 3]\n",
    "        dims = [96, 192, 384, 768]\n",
    "\n",
    "        # Stage별 다운샘플 레이어 (Stem 포함)\n",
    "        self.downsample_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, dims[0], kernel_size=4, stride=4),  # 128 → 32\n",
    "                LayerNorm2d(dims[0])\n",
    "            )\n",
    "        ])\n",
    "        for i in range(3):\n",
    "            self.downsample_layers.append(\n",
    "                nn.Sequential(\n",
    "                    LayerNorm2d(dims[i]),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2)  # 해상도 절반\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ConvNeXt Blocks\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            blocks = [ConvNeXtBlock(dims[i]) for _ in range(depths[i])]\n",
    "            self.stages.append(nn.ModuleList(blocks))\n",
    "\n",
    "        self.norm = LayerNorm2d(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample_layers[0](x)\n",
    "        for i in range(4):\n",
    "            for block in self.stages[i]:\n",
    "                x = block(x)\n",
    "            if i < 3:\n",
    "                x = self.downsample_layers[i+1](x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean([-2, -1])  # GAP\n",
    "        x = self.dropout(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"ConvNeXt-Tiny: Stem(4x4,s4) → Blocks×{3,3,9,3} → GAP → D → FC\")\n",
    "print(ConvNeXtTiny())\n",
    "print(f\"파라미터: {sum(p.numel() for p in ConvNeXtTiny().parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 학습/평가 유틸리티 함수 (H100 최적화)\n",
    "# - GradScaler로 underflow 방지\n",
    "# - autocast로 BF16/FP16 연산\n",
    "# - scheduler/early stopping은 train_model에서 관리\n",
    "# --------------------------------------------\n",
    "# Mixed Precision을 위한 GradScaler (H100 최적화)\n",
    "scaler = GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"Mixed Precision 학습 (H100 최적화)\"\"\"\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # AMP: forward/backward를 자동 캐스팅\n",
    "        with autocast('cuda', enabled=USE_AMP):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 스케일된 그래디언트로 업데이트\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"평가 함수 - Accuracy, F1 (Micro), F1 (Macro) 반환\"\"\"\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            with autocast('cuda', enabled=USE_AMP):\n",
    "                outputs = model(images)\n",
    "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "    \n",
    "    acc = accuracy_score(labels_list, preds)\n",
    "    f1_micro = f1_score(labels_list, preds, average='micro')\n",
    "    f1_macro = f1_score(labels_list, preds, average='macro')\n",
    "    return acc, f1_micro, f1_macro\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    \"\"\"H100 최적화 학습 함수\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, _, _ = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 7:\n",
    "                break\n",
    "    \n",
    "    # 가장 성능 좋은 스냅샷 복원\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "print(\"H100 최적화 학습 함수 정의 완료\")\n",
    "print(\"  - F1 Score: Micro (=Accuracy) + Macro (클래스별 평균)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 반복 홀드아웃 검증 (5회)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 인덱스 기반 부분집합을 transform과 함께 래핑하는 헬퍼\n",
    "# --------------------------------------------\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        image = Image.open(self.base.image_paths[i]).convert('RGB')\n",
    "        return self.transform(image), self.base.labels[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 반복 홀드아웃 실행 함수\n",
    "# - Stratified split으로 3:2(train:val) 분할을 5회 반복\n",
    "# - torch.compile로 추가 최적화 시도 (가능한 경우)\n",
    "# - 테스트 세트는 고정\n",
    "# --------------------------------------------\n",
    "def run_holdout(model_class, model_name, n_repeats=5):\n",
    "    \"\"\"H100 최적화 반복 홀드아웃\"\"\"\n",
    "    results = {'accuracy': [], 'f1_micro': [], 'f1_macro': []}\n",
    "    all_indices = np.arange(len(train_dataset))\n",
    "    \n",
    "    print()  # 구분을 위한 빈 줄\n",
    "    print('='*60)\n",
    "    print(model_name)\n",
    "    print('='*60)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    for i in range(n_repeats):\n",
    "        set_seed(42 + i)\n",
    "        \n",
    "        # 3:2 분할 (train:val)\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            all_indices, test_size=0.4, stratify=train_dataset.labels, random_state=42+i\n",
    "        )\n",
    "        \n",
    "        train_subset = TransformDataset(train_dataset, train_idx, train_transform)\n",
    "        val_subset = TransformDataset(train_dataset, val_idx, test_transform)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_subset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_subset, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "        \n",
    "        model = model_class().to(device)\n",
    "        \n",
    "        # PyTorch 2.x: torch.compile로 추가 최적화 시도\n",
    "        if hasattr(torch, 'compile'):\n",
    "            try:\n",
    "                model = torch.compile(model, mode='reduce-overhead')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        model = train_model(model, train_loader, val_loader, epochs=30)\n",
    "        \n",
    "        acc, f1_micro, f1_macro = evaluate(model, test_loader)\n",
    "        results['accuracy'].append(acc)\n",
    "        results['f1_micro'].append(f1_micro)\n",
    "        results['f1_macro'].append(f1_macro)\n",
    "        print(f\"[{i+1}/{n_repeats}] Acc: {acc:.4f}, F1(Micro): {f1_micro:.4f}, F1(Macro): {f1_macro:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lenet_results = run_holdout(LeNet5, \"LeNet-5 (베이스라인)\")"
  },
  {
   "cell_type": "code",
   "source": "vgg_results = run_holdout(VGGNet, \"VGGNet-style\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_results = run_holdout(ResNetCNN, \"ResNetCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senet_results = run_holdout(SEResNet, \"SEResNet (최고 성능)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "convnext_results = run_holdout(ConvNeXtTiny, \"ConvNeXt-Tiny (고성능)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(results, name):\n",
    "    # 평균/표준편차를 빠르게 확인하기 위한 헬퍼\n",
    "    acc_mean, acc_std = np.mean(results['accuracy']), np.std(results['accuracy'])\n",
    "    f1_micro_mean, f1_micro_std = np.mean(results['f1_micro']), np.std(results['f1_micro'])\n",
    "    f1_macro_mean, f1_macro_std = np.mean(results['f1_macro']), np.std(results['f1_macro'])\n",
    "    print(f\"{name}\")\n",
    "    print(f\"  Accuracy:    {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "    print(f\"  F1 (Micro):  {f1_micro_mean:.4f} ± {f1_micro_std:.4f}\")\n",
    "    print(f\"  F1 (Macro):  {f1_macro_mean:.4f} ± {f1_macro_std:.4f}\")\n",
    "    return acc_mean, acc_std, f1_micro_mean, f1_micro_std, f1_macro_mean, f1_macro_std\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"최종 결과 (Final Results)\")\n",
    "print(\"=\"*60)\n",
    "lenet_stats = print_summary(lenet_results, \"LeNet-5\")\n",
    "print()\n",
    "vgg_stats = print_summary(vgg_results, \"VGGNet\")\n",
    "print()\n",
    "resnet_stats = print_summary(resnet_results, \"ResNetCNN\")\n",
    "print()\n",
    "senet_stats = print_summary(senet_results, \"SEResNet\")\n",
    "print()\n",
    "convnext_stats = print_summary(convnext_results, \"ConvNeXt-Tiny\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 시각화 - 모든 평가 지표 (Accuracy, F1 Micro, F1 Macro) 비교\n# - 한글 폰트 재설정 (그래프 출력 전 확인)\n# - 3개 지표를 하나의 그래프에 그룹으로 표시\n# --------------------------------------------\n\n# 한글 폰트 재설정 (시각화 전 확인)\nimport platform\nif platform.system() == 'Darwin':  # macOS\n    plt.rcParams['font.family'] = 'AppleGothic'\nelif platform.system() == 'Windows':\n    plt.rcParams['font.family'] = 'Malgun Gothic'\nelse:  # Linux\n    # 사용 가능한 한글 폰트 찾기\n    font_list = [f.name for f in fm.fontManager.ttflist if 'Nanum' in f.name or 'Gothic' in f.name]\n    if font_list:\n        plt.rcParams['font.family'] = font_list[0]\nplt.rcParams['axes.unicode_minus'] = False\n\n# 모델별 결과 정리\nmodels = ['LeNet-5', 'VGGNet', 'ResNetCNN', 'SEResNet', 'ConvNeXt']\nall_results = [lenet_results, vgg_results, resnet_results, senet_results, convnext_results]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2a3ff']\n\n# ============================================\n# 그래프 1: 모든 평가 지표 비교 (평균 ± 표준편차)\n# ============================================\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# 1. Accuracy\nacc_means = [np.mean(r['accuracy']) for r in all_results]\nacc_stds = [np.std(r['accuracy']) for r in all_results]\nbars1 = axes[0].bar(models, acc_means, yerr=acc_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\naxes[0].set_title('Accuracy (정확도)', fontsize=12, fontweight='bold')\naxes[0].set_ylim([0.5, 1.0])\naxes[0].set_ylabel('Score')\naxes[0].tick_params(axis='x', rotation=15)\n# 막대 위에 수치 표시\nfor bar, mean, std in zip(bars1, acc_means, acc_stds):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n                 f'{mean:.3f}', ha='center', va='bottom', fontsize=9)\n\n# 2. F1 Score (Micro)\nf1_micro_means = [np.mean(r['f1_micro']) for r in all_results]\nf1_micro_stds = [np.std(r['f1_micro']) for r in all_results]\nbars2 = axes[1].bar(models, f1_micro_means, yerr=f1_micro_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\naxes[1].set_title('F1 Score (Micro)', fontsize=12, fontweight='bold')\naxes[1].set_ylim([0.5, 1.0])\naxes[1].set_ylabel('Score')\naxes[1].tick_params(axis='x', rotation=15)\nfor bar, mean, std in zip(bars2, f1_micro_means, f1_micro_stds):\n    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n                 f'{mean:.3f}', ha='center', va='bottom', fontsize=9)\n\n# 3. F1 Score (Macro)\nf1_macro_means = [np.mean(r['f1_macro']) for r in all_results]\nf1_macro_stds = [np.std(r['f1_macro']) for r in all_results]\nbars3 = axes[2].bar(models, f1_macro_means, yerr=f1_macro_stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\naxes[2].set_title('F1 Score (Macro)', fontsize=12, fontweight='bold')\naxes[2].set_ylim([0.5, 1.0])\naxes[2].set_ylabel('Score')\naxes[2].tick_params(axis='x', rotation=15)\nfor bar, mean, std in zip(bars3, f1_macro_means, f1_macro_stds):\n    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n                 f'{mean:.3f}', ha='center', va='bottom', fontsize=9)\n\nplt.suptitle('Cats vs Dogs - 모델별 성능 비교 (5회 반복 홀드아웃)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('task1_metrics_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 그래프 2: 세 지표를 하나의 그래프에 그룹으로 표시\n# ============================================\nfig, ax = plt.subplots(figsize=(14, 6))\n\nx = np.arange(len(models))\nwidth = 0.25\n\n# 세 지표를 나란히 배치\nbars_acc = ax.bar(x - width, acc_means, width, yerr=acc_stds, label='Accuracy', color='#4ECDC4', capsize=3, edgecolor='black', linewidth=0.5)\nbars_micro = ax.bar(x, f1_micro_means, width, yerr=f1_micro_stds, label='F1 (Micro)', color='#FF6B6B', capsize=3, edgecolor='black', linewidth=0.5)\nbars_macro = ax.bar(x + width, f1_macro_means, width, yerr=f1_macro_stds, label='F1 (Macro)', color='#45B7D1', capsize=3, edgecolor='black', linewidth=0.5)\n\nax.set_xlabel('모델', fontsize=12)\nax.set_ylabel('Score', fontsize=12)\nax.set_title('Cats vs Dogs - 평가 지표별 모델 성능 비교', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(models, fontsize=10)\nax.legend(loc='lower right', fontsize=10)\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\n\n# 수치 표시 (Accuracy만)\nfor bar, mean in zip(bars_acc, acc_means):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{mean:.3f}', ha='center', va='bottom', fontsize=8, rotation=0)\n\nplt.tight_layout()\nplt.savefig('task1_grouped_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 그래프 3: 반복별 성능 추이 (F1 Macro)\n# ============================================\nfig, ax = plt.subplots(figsize=(12, 5))\nx = np.arange(5)\nwidth = 0.15\n\nfor i, (result, model, color) in enumerate(zip(all_results, models, colors)):\n    offset = (i - 2) * width\n    ax.bar(x + offset, result['f1_macro'], width, label=model, color=color, edgecolor='black', linewidth=0.5)\n\nax.set_xlabel('반복 횟수 (Iteration)', fontsize=12)\nax.set_ylabel('F1 Score (Macro)', fontsize=12)\nax.set_title('Cats vs Dogs - 반복별 F1 (Macro) 성능', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels([f'{i+1}회' for i in x], fontsize=10)\nax.legend(loc='lower right', fontsize=9)\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('task1_iteration_f1macro.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================\n# 결과 요약 테이블 출력\n# ============================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"📊 Cats vs Dogs - 최종 결과 요약 (5회 반복 홀드아웃)\")\nprint(\"=\"*80)\nprint(f\"{'모델':<15} {'Accuracy':<20} {'F1 (Micro)':<20} {'F1 (Macro)':<20}\")\nprint(\"-\"*80)\nfor model, result in zip(models, all_results):\n    acc_m, acc_s = np.mean(result['accuracy']), np.std(result['accuracy'])\n    f1mi_m, f1mi_s = np.mean(result['f1_micro']), np.std(result['f1_micro'])\n    f1ma_m, f1ma_s = np.mean(result['f1_macro']), np.std(result['f1_macro'])\n    print(f\"{model:<15} {acc_m:.4f} ± {acc_s:.4f}      {f1mi_m:.4f} ± {f1mi_s:.4f}      {f1ma_m:.4f} ± {f1ma_s:.4f}\")\nprint(\"=\"*80)\nprint(\"\\n※ 참고: 균형 데이터셋에서 F1 (Micro) = Accuracy 입니다.\")\nprint(\"※ F1 (Macro)는 각 클래스(고양이/개)의 F1을 평균하여 클래스별 성능을 균등하게 반영합니다.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 결론\n\n### 모델별 성능 비교\n\n| 모델 | 연도 | 핵심 구조 | 파라미터 | 예상 성능 |\n|------|------|----------|---------|----------|\n| LeNet-5 | 1998 | C(5×5)-Tanh-AvgP | ~560K | 낮음 (베이스라인) |\n| VGGNet | 2014 | [C(3×3)-BN-R]×n-MaxP | ~1.9M | 중간 |\n| ResNetCNN | 2015 | Residual Block (Skip) | ~600K | 높음 |\n| SEResNet | 2017 | SE Block + Residual | ~620K | 높음 |\n| ConvNeXt | 2022 | DWConv(7×7)-LN-GELU | ~28M | 최고 |\n\n※ 실제 성능은 위 시각화 결과 참조\n\n---\n\n### 평가 척도 해석\n\n| 척도 | 계산 방식 | 이 데이터셋에서의 의미 |\n|------|----------|---------------------|\n| **Accuracy** | (TP+TN) / 전체 | 전체 정확도 |\n| **F1 (Micro)** | 전체 TP,FP,FN 합산 | 균형 데이터 → Accuracy와 동일 |\n| **F1 (Macro)** | 클래스별 F1 평균 | 고양이/개 각각의 성능 균등 반영 |\n\n※ 균형 데이터셋에서 F1 (Micro) = Accuracy는 sklearn의 정상 동작입니다.\n\n---\n\n### 모델별 상세 분석\n\n#### 1. LeNet-5 (베이스라인)\n- **구조**: 5×5 큰 커널 + Tanh 활성화 + AvgPool\n- **특징**: 1998년 고전적 CNN, 수업시간에 배운 모델\n- **한계**: 얕은 구조(3 conv)로 128×128 이미지의 복잡한 패턴 학습에 한계\n- **예상 성능**: 가장 낮음\n\n#### 2. VGGNet-style\n- **구조**: 3×3 작은 커널을 깊게 쌓음 (10 conv)\n- **특징**: 3×3 두 번 = 5×5 수용영역, 더 적은 파라미터\n- **개선점**: BN + ReLU로 현대화\n- **한계**: Skip connection 없어 깊어질수록 gradient vanishing 가능\n\n#### 3. ResNetCNN\n- **구조**: Residual Block (H(x) = F(x) + x)\n- **특징**: Skip connection으로 gradient flow 개선\n- **효과**: VGGNet보다 적은 파라미터로 더 깊은 학습 가능\n- **예상 성능**: VGGNet보다 높음\n\n#### 4. SEResNet\n- **구조**: ResNet + SE Block (채널 Attention)\n- **SE Block**: GAP → FC → ReLU → FC → Sigmoid → Scale\n- **효과**: 채널별 중요도 동적 조절 (털 패턴, 귀 모양 등 강조)\n- **예상 성능**: ResNet보다 높음\n\n#### 5. ConvNeXt-Tiny (2022 최신)\n- **구조**: ViT 설계 원칙을 CNN에 적용\n- **핵심 요소**:\n  - Depthwise 7×7 conv (큰 수용영역)\n  - LayerNorm (BatchNorm 대체)\n  - GELU 활성화 (ReLU 대체)\n  - Inverted Bottleneck (채널 4배 확장 후 축소)\n- **특징**: ImageNet에서 ViT, ResNet을 능가하는 성능\n- **예상 성능**: 가장 높음 (파라미터 수 대비 성능 최고)\n\n---\n\n### CNN 발전 흐름 요약\n\n```\nLeNet-5 (1998)     → 기초적 CNN 구조 확립\n       ↓\nVGGNet (2014)      → 작은 커널을 깊게 쌓는 방식\n       ↓\nResNet (2015)      → Skip Connection으로 깊은 학습 가능\n       ↓\nSENet (2017)       → Channel Attention 도입\n       ↓\nConvNeXt (2022)    → Transformer 설계 원칙을 CNN에 적용\n```\n\n### 개 vs 고양이 분류에서 중요한 특징\n\n| 특징 | 개 | 고양이 | 구분 난이도 |\n|------|-----|--------|------------|\n| 귀 형태 | 늘어진/뾰족한 | 삼각형 | 중간 |\n| 코 형태 | 긴 주둥이 | 짧은 주둥이 | 중간 |\n| 눈 형태 | 다양 | 세로 동공 | 쉬움 |\n| 털 패턴 | 다양 | 줄무늬/단색 | 어려움 |\n| 전체 형태 | 다양한 크기 | 비교적 균일 | 중간 |\n\n→ **SE Attention**이 털 패턴, 귀 모양 등 중요한 특징 채널을 강조하는 데 유리\n\n이 프로젝트에서는 CNN의 발전 역사를 따라가며 각 기법의 효과를 실험적으로 비교합니다."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}